<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sams Performance Training</title>
    <link>/</link>
    <description>Recent content on Sams Performance Training</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Matt Sams {year}</copyright>
    <lastBuildDate>Wed, 01 May 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Individualized Athlete Monitoring in R, Part 3 (Interpreting the Predictive Model)</title>
      <link>/post/2018-11-05-r-individual-monitoring-part-3/r-individual-monitoring-part-2/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-11-05-r-individual-monitoring-part-3/r-individual-monitoring-part-2/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hitting-the-ground-running&#34;&gt;Hitting the Ground Running&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#its-predicting-time&#34;&gt;It’s Predicting Time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#identifying-anomalies&#34;&gt;Identifying Anomalies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapping-up&#34;&gt;Wrapping Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;hitting-the-ground-running&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hitting the Ground Running&lt;/h2&gt;
&lt;p&gt;This post assumes you’ve already read &lt;a href=&#34;/post/2018-08-15-r-individual-monitoring/r-individual-monitoring-part-1/&#34; target=&#34;_blank&#34;&gt;part 1&lt;/a&gt; and &lt;a href=&#34;/post/2018-11-05-r-individual-monitoring-part-2/r-individual-monitoring-part-2/&#34; target=&#34;_blank&#34;&gt;part 2&lt;/a&gt; for “Individual Athlete Monitoring in R.” If you haven’t I’d suggest you go back to part 2 for some context for this post. Otherwise, you’re going to be pretty lost. Let’s go.&lt;/p&gt;
&lt;div id=&#34;its-predicting-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;It’s Predicting Time&lt;/h3&gt;
&lt;p&gt;Last time, we built a linear mixed model with the &lt;code&gt;lme4&lt;/code&gt; package called &lt;code&gt;final model&lt;/code&gt;. This model used two categorical fixed effects (season, phase within season) along with five grand mean centered numeric fixed effects (total distance, work rate, hi-speed running distance, time spent in drills, and total practice duration) to predict log transformed rpetl values. We further included athletes nested within season as random intercept effects in the model to reflect that we’re analyzing repeated-measures data where the magnitudes of the relationships between variables may differ between athletes and between seasons. I know that’s a bit of a mouthful, so seriously, go back and look at &lt;a href=&#34;/post/2018-11-05-r-individual-monitoring-part-2/r-individual-monitoring-part-2/&#34; target=&#34;_blank&#34;&gt;part 2&lt;/a&gt; if you’re not sure what I’m talking about. The model output is below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by maximum likelihood  [&amp;#39;lmerMod&amp;#39;]
## Formula: 
## log.rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run +  
##     field.minutes + duration + season + phase
##    Data: rpe.scaled
## 
##      AIC      BIC   logLik deviance df.resid 
##   2460.7   2550.4  -1215.3   2430.7     2915 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.9599 -0.6227  0.0843  0.6488  3.9842 
## 
## Random effects:
##  Groups         Name        Variance Std.Dev.
##  athlete:season (Intercept) 0.02931  0.1712  
##  season         (Intercept) 0.00000  0.0000  
##  Residual                   0.12566  0.3545  
## Number of obs: 2930, groups:  athlete:season, 103; season, 4
## 
## Fixed effects:
##                 Estimate Std. Error t value
## (Intercept)      5.29222    0.03815 138.709
## odometer         0.04029    0.03012   1.338
## work.rate        0.20622    0.02180   9.459
## hi.run           0.03150    0.01170   2.691
## field.minutes    0.06492    0.02000   3.246
## duration         0.31320    0.01110  28.217
## seasonFall 2015 -0.17880    0.04960  -3.605
## seasonFall 2016 -0.40376    0.05327  -7.579
## seasonFall 2017 -0.47707    0.05711  -8.354
## phaseNC          0.07035    0.01740   4.043
## phasePOST        0.02321    0.02188   1.060
## phasePRE         0.32283    0.02331  13.848
## 
## Correlation of Fixed Effects:
##             (Intr) odomtr wrk.rt hi.run fld.mn duratn sF2015 sF2016 sF2017
## odometer    -0.035                                                        
## work.rate    0.033 -0.886                                                 
## hi.run       0.032 -0.420  0.120                                          
## field.mints  0.022 -0.816  0.782  0.193                                   
## duration     0.071 -0.155  0.122 -0.047 -0.257                            
## sesnFll2015 -0.706  0.001 -0.004  0.007 -0.007  0.002                     
## sesnFll2016 -0.646 -0.014  0.001  0.026  0.005  0.018  0.510              
## sesnFll2017 -0.610  0.001  0.002  0.005 -0.005  0.003  0.476  0.443       
## phaseNC     -0.261  0.062 -0.090 -0.060 -0.026 -0.210 -0.007 -0.047 -0.018
## phasePOST   -0.167  0.079 -0.083 -0.162  0.016 -0.129 -0.052 -0.043 -0.035
## phasePRE    -0.212  0.122 -0.107 -0.114 -0.078 -0.242  0.000 -0.032 -0.012
##             phasNC phPOST
## odometer                 
## work.rate                
## hi.run                   
## field.mints              
## duration                 
## sesnFll2015              
## sesnFll2016              
## sesnFll2017              
## phaseNC                  
## phasePOST    0.474       
## phasePRE     0.522  0.366
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under normal circumstances, you would probably be comparing several methods to model the relationship between external load metrics and rpetl. In that case, you would go through a cross-validation process similar to &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Carey_2016&#34;&gt;1&lt;/a&gt;]&lt;/span&gt;, where you split your data into training and test sets to determine which method is most effective in predicting previously-unseen data. We’ll leave that for some other time, though. For now, let’s attempt to tackle a real-life problem: using our model, can we identify anomalous rpetl values? I’m going to split the data so that we’re attempting to predict rpetl values for the final three weeks of the 2017 season. I’ll also re-create the model based on this trimmed dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a training set
training.data &amp;lt;- rpe.scaled[date &amp;lt; &amp;quot;2017-10-16&amp;quot;]

prediction.data &amp;lt;- rpe.scaled[date &amp;gt;= &amp;quot;2017-10-16&amp;quot;]

# New model based on training.data
final.model &amp;lt;- lmer(log.rpetl ~ (1|season/athlete) + odometer + work.rate + hi.run + 
                         field.minutes + duration + season + phase, 
                       data = training.data, REML = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s important to note the scaled predictors in &lt;code&gt;training.data&lt;/code&gt; aren’t &lt;em&gt;quite&lt;/em&gt; correct because the original scaling process included the data from &lt;code&gt;prediction.data&lt;/code&gt;. In the real world, your training data will lag behind the data you want to predict by a few days or a few weeks. The amount of lag is up to you as I haven’t found any satisfying answers in reading about predictive modeling. You could easily set up a script that would update your training dataset every few days to keep your model up-to-date. You could, however, base &lt;code&gt;prediction.data&lt;/code&gt;’s scaled coefficients on the entire dataset to ensure the scaled values accurately reflect the data. Anyway, let’s start by using our model to predict rpetl values for &lt;code&gt;training.data&lt;/code&gt;. Remember, we log transformed rpetl last time, so we’ll need to back-transform the predictions via &lt;code&gt;exp()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict log-transformed rpetl values from our final model
training.data$log.prediction &amp;lt;- predict(final.model, training.data)

# Back-transform the predictions to raw rpetl values
training.data[, prediction := exp(log.prediction)]

knitr::kable(training.data[1:10, c(&amp;quot;athlete&amp;quot;, &amp;quot;date&amp;quot;, &amp;quot;rpetl&amp;quot;, &amp;quot;prediction&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
athlete
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rpetl
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
prediction
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jonathan Sanchez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
592.1312
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Dhaki al-Naim
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
623.3140
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Corey Klocker
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
578.5048
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Issac Martinez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
418.6855
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Phoenix Lee
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
360
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
470.6439
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Carroll Bond
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
540
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
598.4328
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Neulyn Whyte
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
720
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
684.5372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jonathan Sanchez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
335.1269
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bobby Landry
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
270
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
351.3422
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Daniel Thompson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
270
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
309.3395
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Notice the predictions aren’t spot-on. We can quantify our model’s prediction error at both the group level and individual level. Model error is typically reported as mean absolute error (MAE) or root mean square error (RMSE). &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Ward_2018&#34;&gt;3&lt;/a&gt;]&lt;/span&gt; recommend using a combination of the between-subject and within-subject RMSE (a.k.a. root mean square deviation) to set limits and confidence intervals for truly anomalous prediction errors. A drawback to RMSE is that it penalizes outliers more heavily compared to MAE. Since we’re interested in identifying outliers in this case, we might examine the MAE instead. But in the words of the Old El Paso commercial, “Por que no los dos?” We can examine both approaches pretty easily with the &lt;code&gt;caret&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Group RMSE
training.data[, .(group.RMSE = RMSE(prediction, rpetl))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    group.RMSE
## 1:   72.43846&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Individual RMSE
training.data[, .(individual.RMSE = RMSE(prediction, rpetl)), by = athlete]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     athlete individual.RMSE
##  1:        Jonathan Sanchez        79.65152
##  2:           Dhaki al-Naim        98.24133
##  3:           Corey Klocker        66.35323
##  4:          Issac Martinez        71.24416
##  5:             Phoenix Lee        96.51796
##  6:            Carroll Bond        84.44877
##  7:            Neulyn Whyte        73.54454
##  8:            Bobby Landry        69.01649
##  9:         Daniel Thompson        74.79385
## 10:          Ethan Phouminh        61.53159
## 11:      Brandon Mclaughlin        82.08851
## 12:           Henry Douglas        57.64887
## 13:             Brandon Tan        54.92733
## 14:            Laten Absher        58.61063
## 15:             John Mowers        68.33314
## 16:        Jeremy Pribbenow        90.63118
## 17:               Leo Ortiz        61.26317
## 18:           Richard Bruff        99.38963
## 19:       Naseer el-Saladin        63.81937
## 20:        Zackarie Broeren        63.17699
## 21:              Erik Dores        87.85881
## 22:        Fawzi al-Shaheed        99.88159
## 23:         Joseph Johnston        68.37198
## 24:         Mashal el-Tahir        84.34851
## 25:          Blake Graybill       117.84725
## 26: Nicholas Rayas Coronado        63.69189
## 27:          Augustus Gallo        82.49120
## 28:       Tammaam al-Sheikh        78.90438
## 29:          Sadi el-Hamady        55.18857
## 30:          Zachary Nestor        64.08365
## 31:             Amru el-Zia        47.99908
## 32:          Dominik Gibson        73.62863
## 33:             Sung-Jin Le        93.70942
## 34:             Tyrone Teka        88.54565
## 35:          Carlos Vasquez        58.78742
## 36:        Benjamin Beverly        63.96766
## 37:              Tyler Choo        44.67102
## 38:          Calvin Charlie        74.99400
## 39:           Braden Bliler        69.91302
## 40:          Quentin Mooney        64.44706
## 41:          Bakar el-Burki        61.10722
## 42:               Dan Munoz        50.43102
## 43:        Matthew Futamata       103.58369
## 44:            Isaac Conway        84.91036
## 45:          Nicholas Mckay        69.71548
## 46:           Scottie Salaz        57.65266
## 47:             Magnus Yang        58.41908
## 48:         Muaaid el-Kamel        67.63561
## 49:             Ruben Ayala        47.00549
## 50:           Jordan Hughes        63.20713
## 51:          Cuauhtemoc Box        67.61856
## 52:          Yusri al-Abdoo        62.40452
## 53:            Ron Quintana        63.10565
## 54:     Samuel Llanas Brito        33.69042
##                     athlete individual.RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Group MAE
training.data[, .(group.MAE = MAE(prediction, rpetl))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    group.MAE
## 1:  50.79352&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Individual MAE
training.data[, .(individual.MAE = MAE(prediction, rpetl)), by = athlete]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     athlete individual.MAE
##  1:        Jonathan Sanchez       57.57782
##  2:           Dhaki al-Naim       68.45952
##  3:           Corey Klocker       47.82331
##  4:          Issac Martinez       54.65807
##  5:             Phoenix Lee       69.40985
##  6:            Carroll Bond       63.20898
##  7:            Neulyn Whyte       56.20448
##  8:            Bobby Landry       50.03018
##  9:         Daniel Thompson       56.23135
## 10:          Ethan Phouminh       47.88505
## 11:      Brandon Mclaughlin       60.74419
## 12:           Henry Douglas       41.67811
## 13:             Brandon Tan       45.46249
## 14:            Laten Absher       47.92199
## 15:             John Mowers       50.86440
## 16:        Jeremy Pribbenow       60.24945
## 17:               Leo Ortiz       43.53623
## 18:           Richard Bruff       72.82061
## 19:       Naseer el-Saladin       52.18722
## 20:        Zackarie Broeren       42.95711
## 21:              Erik Dores       56.30851
## 22:        Fawzi al-Shaheed       62.52946
## 23:         Joseph Johnston       48.36695
## 24:         Mashal el-Tahir       58.73721
## 25:          Blake Graybill       80.55885
## 26: Nicholas Rayas Coronado       43.80600
## 27:          Augustus Gallo       58.77337
## 28:       Tammaam al-Sheikh       64.73494
## 29:          Sadi el-Hamady       40.39463
## 30:          Zachary Nestor       50.79137
## 31:             Amru el-Zia       33.30804
## 32:          Dominik Gibson       53.35034
## 33:             Sung-Jin Le       59.76749
## 34:             Tyrone Teka       60.98581
## 35:          Carlos Vasquez       45.81151
## 36:        Benjamin Beverly       44.98752
## 37:              Tyler Choo       34.61203
## 38:          Calvin Charlie       60.85675
## 39:           Braden Bliler       52.30069
## 40:          Quentin Mooney       50.99086
## 41:          Bakar el-Burki       51.60180
## 42:               Dan Munoz       43.30896
## 43:        Matthew Futamata       72.77470
## 44:            Isaac Conway       51.32585
## 45:          Nicholas Mckay       52.86036
## 46:           Scottie Salaz       42.19793
## 47:             Magnus Yang       39.66141
## 48:         Muaaid el-Kamel       55.96923
## 49:             Ruben Ayala       30.08009
## 50:           Jordan Hughes       55.71311
## 51:          Cuauhtemoc Box       48.89419
## 52:          Yusri al-Abdoo       41.74455
## 53:            Ron Quintana       42.21368
## 54:     Samuel Llanas Brito       25.39538
##                     athlete individual.MAE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s pretty interesting to examine the spread of the prediction errors among athletes. I would wager we could improve the model’s predictive accuracy by accounting for pre-training wellness, previous match outcome, injury status, MD minus, and other contextual factors, but I think a RMSE of ~ 72 units (range 33 - 118) and MAE of 51 units (range 25 - 81) will be acceptable for today’s example. Further, an approach similar to &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Vandewiele_2017&#34;&gt;2&lt;/a&gt;]&lt;/span&gt;, where anomalous points produced by sickness and injury are retroactively removed, might be another way to improve the model’s predictive ability. Unfortunately, we didn’t collect contextual data like that, so we’re stuck with what we have.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying-anomalies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identifying Anomalies&lt;/h2&gt;
&lt;p&gt;First, let’s add our between- and within-subject RMSE and MAE to &lt;code&gt;prediction.data&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Group RMSE
group.RMSE &amp;lt;- training.data[, RMSE(prediction, rpetl)]

# Individual RMSE
individual.RMSE &amp;lt;- training.data[, .(individual.RMSE = RMSE(prediction, rpetl)), 
                             by = athlete]

# Group MAE
group.MAE &amp;lt;- training.data[, MAE(prediction, rpetl)]

# Individual MAE
individual.MAE &amp;lt;- training.data[, .(individual.MAE = MAE(prediction, rpetl)), 
                                by = athlete]

# Add model predictions to prediction.data
prediction.data$log.prediction &amp;lt;- predict(final.model, prediction.data)

prediction.data[, prediction := exp(log.prediction)]

# Add group MAE and RMSE to prediction.data
prediction.data[, &amp;quot;:=&amp;quot; (prediction.error = rpetl - prediction, 
                        group.RMSE = group.RMSE,
                        group.MAE = group.MAE)]

# Add individual RMSE and MAE via merge()
prediction.data &amp;lt;- merge(prediction.data, individual.RMSE, sort = FALSE)

prediction.data &amp;lt;- merge(prediction.data, individual.MAE, sort = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we’ve added the RMSE and MAE to &lt;code&gt;prediction.data&lt;/code&gt;, we can visualize them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# RMSE plot
ggplot(prediction.data, aes(x = date, y = prediction.error)) +
  geom_pointrange(aes(ymin = prediction.error - individual.RMSE,
                      ymax = prediction.error + individual.RMSE)) +
  geom_ribbon(aes(ymin = -group.RMSE,
                  ymax = group.RMSE), alpha = 0.2) +
  theme_bw() + facet_wrap(~athlete, scales = &amp;quot;free_y&amp;quot;) +
  labs(title = &amp;quot;Predictions vs. Actual (RMSE)&amp;quot;,
       x = &amp;quot;Date&amp;quot;,
       y = &amp;quot;Actual - Predicted RPETL (AU)&amp;quot;) +
  geom_hline(aes(yintercept = 0), linetype = &amp;quot;dotdash&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-3/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# MAE plot
ggplot(prediction.data, aes(x = date, y = prediction.error)) +
  geom_pointrange(aes(ymin = prediction.error - individual.MAE,
                      ymax = prediction.error + individual.MAE)) +
  geom_ribbon(aes(ymin = -group.MAE,
                  ymax = group.MAE), alpha = 0.2) +
  theme_bw() + facet_wrap(~athlete, scales = &amp;quot;free_y&amp;quot;) +
  labs(title = &amp;quot;Predictions vs. Actual (MAE)&amp;quot;,
       x = &amp;quot;Date&amp;quot;,
       y = &amp;quot;Actual - Predicted RPETL (AU)&amp;quot;) +
  geom_hline(aes(yintercept = 0), linetype = &amp;quot;dotdash&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-3/index_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s pretty difficult to make out what’s happening at the group level, so let’s zoom in on a single athlete (Samuel Llanas Brito).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-3/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-3/index_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For Samuel, both the RMSE and MAE methods are in relative agreement with two sessions drastically above predicted, three sessions just above the error limits, and one session well below what was predicted. Having access to the original data, I can tell you the first anomalous point comes from the athlete’s first practice back from a contact injury sustained 9 days before. It’s safe to assume that contributed to the anomalous score. Similarly, the second point came after his first match involvement following his injury. We can assume some lingering match fatigue may have contributed, which is also likely reflected in his above-prediction scores on Oct. 31st and Nov. 2nd. For him, an altered training setup (bike/pool fitness, play as a neutral, etc.) with more gradual RTP may have been preferrable to immediate full involvement. Of course, it’s easy to say that now, but examining this data retroactively can also help us understand how we can improve our RTP process in the future.&lt;/p&gt;
&lt;p&gt;As for the extreme over-prediction on November 3rd, that was an intrasquad during a bye week. Having been with the team through a number of these over the years, late-season intrasquads tended to be more laid back and ended with a low-stakes round of penalty kicks where the athletes inentionally attempted to distract the athlete taking the PK. The relaxed atmosphere and extended low-intensity (and sometimes silly) round of PKs probably contributed to the lower-than-predicted rpetl values. And if you look back at the other athletes’ data, you’ll notice a similar trend for most of the team. Again, situations like this are where adding additional complexity to the model (percent of practice time spent in drills, maybe? Feel free to experiment.) may improve its predictive power.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;enough.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think this is a good place to stop. Hopefully parts 2 and 3 have given you a decent introduction to using mixed model predictions as part of your athlete monitoring. This post wasn’t comprehensive, by any means, and probably left out some parts of the process. So if you’re still a little unsure of where to go from here, feel free to hit me up on Twitter or via email and I’ll get back to you ASAP. Also, I know I didn’t address using the fixed effects’ coefficients for athletes with limited data; if you would like to see my thoughts on how to implement that, let me know and I’ll add an addendum to this post. Finally, feel free to share your own ideas on how to improve the predictive accuracy of the model. I always love hearing others’ ideas!&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Carey_2016&#34;&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;em&gt;Carey DL, Ong K, Morris ME, Crow J, Crossley KM. &lt;/em&gt;Predicting ratings of perceived exertion in australian football players: Methods for live estimation. International Journal of Computer Science in Sport 2016; 15: 64–77 Available from: &lt;a href=&#34;https://doi.org/10.1515/ijcss-2016-0005&#34;&gt;https://doi.org/10.1515/ijcss-2016-0005&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Vandewiele_2017&#34;&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;Vandewiele G, Geurkink Y, Lievens M, Ongenae F, Turck FD, Boone J. &lt;/em&gt;Enabling training personalization by predicting the session rate of perceived exertion (sRPE). 2017;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Ward_2018&#34;&gt;
&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;em&gt;Ward P, Coutts AJ, Pruna R, McCall A. &lt;/em&gt;Putting the “i” back in team. International Journal of Sports Physiology and Performance 2018; 1–14 Available from: &lt;a href=&#34;https://doi.org/10.1123/ijspp.2018-0154&#34;&gt;https://doi.org/10.1123/ijspp.2018-0154&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Individualized Athlete Monitoring in R, Part 2 (Building a Predictive Model)</title>
      <link>/post/2018-11-05-r-individual-monitoring-part-2/r-individual-monitoring-part-2/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-11-05-r-individual-monitoring-part-2/r-individual-monitoring-part-2/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#whoops&#34;&gt;Whoops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-refresher&#34;&gt;A Refresher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#anomaly-detection&#34;&gt;Anomaly Detection&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rm-anova-vs.-mixed-modeling&#34;&gt;RM ANOVA vs. Mixed Modeling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-a-model&#34;&gt;Building a Model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#visualizing-the-data&#34;&gt;Visualizing the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-the-model&#34;&gt;Building the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapping-up&#34;&gt;Wrapping Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;whoops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Whoops&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;alive.png&#34; /&gt;
Alright, so maybe I’m a liar and part 2 took…a while…to come to fruition. Sorry about that. In my defense, I’ve traveled to four different countries for work, worked on sprint step analysis for &lt;a href=&#34;https://twitter.com/DrBradDeWeese&#34; target=&#34;_blank&#34;&gt;@DrBradDeWeese&lt;/a&gt; (hopefully we can share some of that soon), and bought a new laptop without any of my blogging packages installed so at least cut me a &lt;em&gt;little&lt;/em&gt; slack. Now, where were we…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-refresher&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Refresher&lt;/h2&gt;
&lt;p&gt;I started off last time by mentioning “Putting the ‘I’ Back in Team” &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Ward_2018&#34;&gt;6&lt;/a&gt;]&lt;/span&gt;. If you haven’t read it, I would recommend starting there before going any further. There were three topics of conversation in the paper: MBI (Imma let everyone finish arguing before wading into that), SPC (last post), and a version of anomaly detection via mixed modeling (this post). If you’re interested in further background information on what I’ll be discussing today, here are the papers that originally sent me down the rabbit hole: &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Bartlett_2017&#34;&gt;1&lt;/a&gt;,&lt;a href=&#34;#ref-Carey_2016&#34;&gt;2&lt;/a&gt;,&lt;a href=&#34;#ref-Vandewiele_2017&#34;&gt;5&lt;/a&gt;]&lt;/span&gt;. Now, you’ll probably look at those papers and say, “Matt, those have nothing to do with mixed models,” and you wouldn’t be wrong. But they gave me some ideas for model building and how to define an outlier/anomaly (and next post will probably look at using non-linear methods to predict sRPE) so give them a read if you feel so inclined.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anomaly-detection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Anomaly Detection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;time-anomaly.png&#34; /&gt;
The concept of anomaly detection is pretty straightforward: Given a dataset, can we detect data that are significantly different from our predictions? The specific field will then dictate what we do with this anomalous data. For instance, credit card fraud detection is probably the most well-known example of anomaly detection–your spending habits (typical purchase price, location, type of purchase, etc.) are used to build an individualized consumer profile. Purchases that fall significantly outside that profile (say, making a purchase in China without notifying your bank…I didn’t do that, for the record) are flagged and automatically rejected. In the case of my credit card company, the transaction is declined and I receive both a text and email asking if that was actually me making the purchase. Once I confirm my identity hasn’t been stolen I can have the merchant run my card again without issue, and the credit card company has new data to build a more robust consumer profile that slowly integrates my new (well, old but in a new country) spending habits into the model.&lt;/p&gt;
&lt;p&gt;In reality, if you’ve been using traditional statistics, MBI, or SPC to identify outliers for follow-up in your monitoring data, you’ve been performing your own version of anomaly detection. That is, you’re looking for data outside the norm or what you would predict–an anomaly if you will. While SPC is great for taking an individualized approach to detecting changes in athletes’ physical characteristics, fatigue state, etc., one of the main issues for individual athlete monitoring is that SPC has a lead time; you need to collect a number of observations on an athlete before you can build a profile of what’s “normal.” Unfortunatley, sport isn’t conducive to “sit and wait,” and non-compliance, absence, and injury make building an individual profile even harder. That’s where tools like mixed models step in. Instead of relying only on individual data (SPC) or pooled data (simple or multiple regression), mixed models attempt to strike a balance between individualization and the “widsom of the crowd.” In theory, it’s the best of both worlds. Athletes with a ton of historical data will have more individualized random effects to complement the model’s fixed effects, whereas we can lean more heavily on the model’s fixed effects for athletes with less data (more on fixed and random effects in just a second).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;both-worlds.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;rm-anova-vs.-mixed-modeling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;RM ANOVA vs. Mixed Modeling&lt;/h3&gt;
&lt;p&gt;In the case of longitudinal analyses, mixed models (or you might see them called linear mixed models) are very similar to the garden-variety repeated-measures ANOVA you’re likely familiar with: a model is built to predict a dependent variable from an independent variable / series of independent variables (between-subjects effects, now called fixed effects) while also accounting for individual variation (within-subjects effects, now part of the random effects). There are some distinct differences between the two appraoches, though. For one, mixed models don’t require complete or balanced datasets. That is, athletes with missing data won’t need to be thrown out or have missing values imputed. This fact alone makes mixed models incredibly powerful tools for model building in sports science. Another important difference between the two approaches is how random effects work. In RM ANOVA, we can specify we’re analyzing data for the same individuals over time, but that’s about it. This can be a problem in the data we collect, where differences can exist between phases of the season (pre-season vs. non-conference vs. conference in the case of college sports) or between seasons (different playstyles, maturation, improved fitness levels, new coach, etc.). RM ANOVA isn’t able to account for this clustering or nesting of variables (note: we can model the interaction between these variables via fixed effects, but that’s a bit different), whereas mixed model random effects are. And let me tell you, random effects can get pretty complicated with especially complex designs, but I’ll leave that for someone smarter to explain. For instance, you can check out &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Field_2012&#34;&gt;3&lt;/a&gt;]&lt;/span&gt; or &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Gelman_2006&#34;&gt;4&lt;/a&gt;]&lt;/span&gt;. The latter contains everything you could ever want to know about mixed models and more…much more, while chapters 13, 14, and 19 of &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Field_2012&#34;&gt;3&lt;/a&gt;]&lt;/span&gt; will equip you with what you need to know in an easier to digest format.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building a Model&lt;/h2&gt;
&lt;p&gt;As always, it’s much easier to show than tell with this stuff, so how about an example. You can find the data for this post &lt;a href=&#34;prediction-data.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,930 x 13
##    athlete date       season session activity field.minutes odometer
##    &amp;lt;fct&amp;gt;   &amp;lt;date&amp;gt;     &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;            &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;
##  1 Jonath~ 2014-08-13 Fall ~       1 Soccer 1          75.3     7201
##  2 Dhaki ~ 2014-08-13 Fall ~       1 Soccer 1          69.5     7300
##  3 Corey ~ 2014-08-13 Fall ~       1 Soccer 1          76.6     7586
##  4 Issac ~ 2014-08-13 Fall ~       1 Soccer 1          80.7     3880
##  5 Phoeni~ 2014-08-13 Fall ~       1 Soccer 1          59.1     5774
##  6 Carrol~ 2014-08-13 Fall ~       1 Soccer 1          80.7     7807
##  7 Neulyn~ 2014-08-13 Fall ~       1 Soccer 1          69.6     7617
##  8 Jonath~ 2014-08-14 Fall ~       1 Soccer 1          82.3     3962
##  9 Bobby ~ 2014-08-14 Fall ~       1 Soccer 1          82.1     3792
## 10 Daniel~ 2014-08-14 Fall ~       1 Soccer 1          79.3     3671
## # ... with 2,920 more rows, and 6 more variables: work.rate &amp;lt;dbl&amp;gt;,
## #   plyr.ld &amp;lt;int&amp;gt;, hi.run &amp;lt;int&amp;gt;, duration &amp;lt;int&amp;gt;, rpetl &amp;lt;int&amp;gt;, phase &amp;lt;fct&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
athlete
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
season
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
session
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
activity
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
field.minutes
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
odometer
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
work.rate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
plyr.ld
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
hi.run
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
duration
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rpetl
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
phase
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jonathan Sanchez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75.30000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7201
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
95.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
690
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1440
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Dhaki al-Naim
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.46667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
105.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
592
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1129
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Corey Klocker
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76.61667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7586
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
99.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
660
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Issac Martinez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80.66667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
379
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Phoenix Lee
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
59.08333
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5774
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
97.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
539
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
718
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
360
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Carroll Bond
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80.66667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7807
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
96.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
714
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1080
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
540
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Neulyn Whyte
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.65000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7617
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
109.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
718
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
720
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jonathan Sanchez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82.33333
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3962
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
435
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
314
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
450
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bobby Landry
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82.06667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3792
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
439
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
370
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
270
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Daniel Thompson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2014-08-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fall 2014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Soccer 1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79.28333
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3671
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
381
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
132
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
270
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The full file contains four seasons’ worth of practice sRPE and GPS data. I’ve already done some data cleaning to remove data entry errors (about 40 records total across the four seasons; most were games incorrectly coded as practice), so you should be cognizant of the fact your data won’t always be this “pretty.” Consistent data collection and entry protocols can help mitigate some of these problems, but it’s always good to double check your data prior to modeling or making any decisions based on your models. We’ll use the data to build and test a predictive model for sRPE-derived training load (sRPE * duration; rpetl in the file) before applying it similarly to &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Ward_2018&#34;&gt;6&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gps-output.png&#34; /&gt;
If you’ve ever seen a GPS output file, you’ll know they can be pretty overwhelming. When I was with the soccer team, Catapult Sprint could spit out something like 250 variables (I think OpenField has even more). We exported 75; we actually looked at 7. Those other 70-ish variables were of the just-in-case variety–the variables you &lt;em&gt;may&lt;/em&gt; want to look at one day, but realistically you know you’ll never give them a second glance. But hey, you have them. Anyway, of those seven variables, research has shown there’s a strong correlation between sRPE, total distance, and Player Load (PL) and a nearly perfect correlation between total distance and PL. We used both when it came to progamming our practice sessions because they do provide slightly different information, but from a model-building standpoint the nearly perfect correlation (aka, multicollinearity) between total distance and PL is problematic. It will bias the model coefficients and affect our ability to interpret the model and individual variable importance within the model. So step one in our process will be to examine the correlation coefficients between the variables in our dataset. We can examine the relationships between our variables very easily with &lt;code&gt;cor()&lt;/code&gt; from the base stats package and the &lt;code&gt;corrplot&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# import the data
rpe.data &amp;lt;- read.csv(&amp;quot;prediction-data.csv&amp;quot;)

# convert the data to data.table format
rpe.data &amp;lt;- data.table(rpe.data)

# conver the date column to date format
rpe.data[, date := as.Date(date, format = &amp;quot;%m/%d/%Y&amp;quot;)]

# Convert all predictor variables to numeric; this will save us a lot of heartache later
rpe.data[, c(&amp;quot;field.minutes&amp;quot;, &amp;quot;odometer&amp;quot;, &amp;quot;work.rate&amp;quot;, &amp;quot;plyr.ld&amp;quot;, &amp;quot;hi.run&amp;quot;, &amp;quot;duration&amp;quot;) :=
           lapply(.SD, as.numeric), .SDcols = field.minutes:duration]

# visualize the correlation matrix from cor()
corrplot(corr = cor(rpe.data[, field.minutes:rpetl]), 
         method = &amp;quot;number&amp;quot;, type = &amp;quot;upper&amp;quot;, diag = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Just as suspected, we have a nearly perfect correlation (0.94) between total distance and PL. A general rule of thumb I’ve seen when it comes to multicollinearity is a correlation coefficient &amp;gt; 0.8 or 0.9, so our 0.94 between total distance and PL definitely fits the bill. In such cases, you could choose either variable with essentially interchangeable results. For sake of consistency with other research, I’m going to use total distance, but you’re welcome to re-run the model with PL instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Drop player load from the data frame
rpe.data$plyr.ld &amp;lt;- NULL

# re-visualize the data
corrplot(corr = cor(rpe.data[, odometer:rpetl]), 
         method = &amp;quot;number&amp;quot;, type = &amp;quot;upper&amp;quot;, diag = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s better. Let’s get to model building.&lt;/p&gt;
&lt;div id=&#34;visualizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualizing the Data&lt;/h3&gt;
&lt;p&gt;The first step in the model building process is always to examine your data visually. Visualizations help us understand what we’re working with and can aid us in fitting models that are appropriate for the dataset. Let’s start off by plotting histograms of the pooled rpetl values and rpetl for each season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot an overall histogram of the rpetl data
ggplot(rpe.data, aes(x = rpetl)) + geom_histogram() + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot individual histograms for each season
ggplot(rpe.data, aes(x = rpetl)) + geom_histogram() + facet_wrap(~season) + 
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In both views of the data, there’s some pretty severe positive skew going on. If you’re familiar with common statistical practices in sports science, you already know how we’re going to deal with this…but humor me for now, please.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and plot the relationships between each of the potential predictor variables and rpetl. Each colored line represents the regression line for its respective season. We’ll leverage the &lt;code&gt;gridExtra&lt;/code&gt; package to combine the individual plots into a single graphic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpe.minutes &amp;lt;- ggplot(rpe.data, aes(x = field.minutes, y = rpetl)) + geom_point() + 
  geom_smooth(aes(colour = season), method = &amp;quot;lm&amp;quot;, se = FALSE) + theme_bw()

rpe.odometer &amp;lt;- ggplot(rpe.data, aes(x = odometer, y = rpetl)) + geom_point() + 
  geom_smooth(aes(colour = season), method = &amp;quot;lm&amp;quot;, se = FALSE) + theme_bw()

rpe.rate &amp;lt;- ggplot(rpe.data, aes(x = work.rate, y = rpetl)) + geom_point() + 
  geom_smooth(aes(colour = season), method = &amp;quot;lm&amp;quot;, se = FALSE) + theme_bw()

rpe.hirun &amp;lt;- ggplot(rpe.data, aes(x = hi.run, y = rpetl)) + geom_point() + 
  geom_smooth(aes(colour = season), method = &amp;quot;lm&amp;quot;, se = FALSE) + theme_bw()

rpe.duration &amp;lt;- ggplot(rpe.data, aes(x = duration, y = rpetl)) + geom_point() + 
  geom_smooth(aes(colour = season), method = &amp;quot;lm&amp;quot;, se = FALSE) + theme_bw()

gridExtra::grid.arrange(rpe.minutes, rpe.odometer, rpe.rate, rpe.hirun, rpe.duration)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This time, pay attention to the shape of the relationships (using &lt;code&gt;colour&lt;/code&gt; on the plots was a jumbled mess, so use your imagination or experiment with facetting by season for each variable). In each case, as the values of the variables increase, the spread of the resultant rpetl values also increases. Again, you probably know how we’re going to deal with this, but let’s start off with some basic models first.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Building the Model&lt;/h3&gt;
&lt;p&gt;First, let’s get acquainted with the syntax we’ll be using. I’m using the &lt;code&gt;lme4&lt;/code&gt; package today, but &lt;code&gt;nlme&lt;/code&gt; is also extremely popular (and can do a few things &lt;code&gt;lme4&lt;/code&gt; can’t). If you’ve used &lt;code&gt;lm()&lt;/code&gt; or &lt;code&gt;aov()&lt;/code&gt;, the syntax is virtually identical. You’ll write an equation with the left and right-hand sides separated by “~”. The dependent variable is on the LHS, whereas the predictors are on the RHS. The major difference from &lt;code&gt;lm()&lt;/code&gt; will be the inclusion of the random effect(s). They should be pretty easy to spot in the following formula.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Build the random intercept-only model
random.intercept.model &amp;lt;- lmer(rpetl ~ (1|season/athlete), 
                               data = rpe.data, REML = FALSE)

summary(random.intercept.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by maximum likelihood  [&amp;#39;lmerMod&amp;#39;]
## Formula: rpetl ~ (1 | season/athlete)
##    Data: rpe.data
## 
##      AIC      BIC   logLik deviance df.resid 
##  36342.4  36366.3 -18167.2  36334.4     2926 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1801 -0.6797 -0.2002  0.4533  4.5445 
## 
## Random effects:
##  Groups         Name        Variance Std.Dev.
##  athlete:season (Intercept)   586.5   24.22  
##  season         (Intercept)  1093.5   33.07  
##  Residual                   13801.3  117.48  
## Number of obs: 2930, groups:  athlete:season, 103; season, 4
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   201.29      16.89   11.92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is called the random intercept-only model because, well, that’s the only thing in the model. In &lt;code&gt;lmer&lt;/code&gt; random effects are defined with &lt;code&gt;(random slope|random intercept)&lt;/code&gt;. This is where things get really complicated, and to be honest, I don’t completely understand all the intricacies. Regardless, the general gist is that we can model differences in both the level and slope of the relationship between the predictor variables and rpetl.&lt;/p&gt;
&lt;p&gt;In the above model, we’ve defined athlete nested within season as a random intercept effect. That means the model assumes the slope of the relationship between the predictors and sRPE is the same across athletes, but the intercept of the model for each athlete is in a different location each season. This makes sense when you think about it. We know sRPE is a “global” indicator of training intensity; that is, it’s affected by both the physiological and psychological state of the athlete (fitness level, fatigue, outside stressors, etc.). So we would expect the resultant rpetl to differ in magnitude between athletes and to differ within athletes across seasons. We can see this in the example plot below. Notice a majority of the athletes’ regression lines are in different locations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could also have a random slope-only model. In that case, the slope of the relationship between the predictors and the DV is allowed to vary, whereas the intercepts are not. In reality, this type of model would make little sense most of the time as differences in slopes would typically lead to differences in intercepts as well.&lt;/p&gt;
&lt;p&gt;And finally, we can model both random slopes and random intercepts. In this case, both the level and slope of the relationship are allowed to differ. There is some evidence to support this approach as &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Bartlett_2017&#34;&gt;1&lt;/a&gt;]&lt;/span&gt; showed variable importance differs across athletes, but modeling something like that is well beyond an introduction to the method. We’ll stick to a random intercept-only model for today.&lt;/p&gt;
&lt;div id=&#34;adding-fixed-effects&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Adding Fixed Effects&lt;/h4&gt;
&lt;p&gt;Model building is a pretty controversial subject. Different approaches exist, including entry of all variables into the model at once, sequential entry of variables into the model, and stepwise entry and deletion of the variables to find the model of best fit. The latter is especially controversial and can be seen as fishing for the best model. It is possible for this approach to over-fit the data and to make little sense from a practical standpoint (statistical significance is prioritized over practical significance), but it tends to be the go-to approach in building predictive models. We can overcome this approach’s shortcomings, though, by 1) using variables that have empirical support (e.g. total distance, heart rate load, pre-training wellness, etc.), 2) performing stepwise analysis on blocks of fixed effects (aka, feature sets; see &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Carey_2016&#34;&gt;2&lt;/a&gt;]&lt;/span&gt; for an example), and 3) using cross-validation (siccing our models on previously unseen data to compare their abilities to generalize to new data). We’ll get into cross-validation some other time.&lt;/p&gt;
&lt;p&gt;We already have our block of fixed effects to test (season, training phase, time spent active in drills [field.minutes], total distance, work rate, hi-speed running distance [&amp;gt; 14.4 kph], and total practice duration), so we can begin adding them to the model and testing their overall effect. Updating models can be done a couple different ways in R. For one, you can manually write the code each time (see the above &lt;code&gt;lme4&lt;/code&gt; code as an example), or you can leverage the &lt;code&gt;update()&lt;/code&gt; function. I’ll be using &lt;code&gt;update()&lt;/code&gt;, so here’s the skinny on how it works: &lt;code&gt;update()&lt;/code&gt; needs two pieces of information, 1) the model you want to update and 2) how you want to update it. We defined the random intercept model earlier, so we would tell the function we want to update random.intercept.model by calling &lt;code&gt;update(random.intercept.model)&lt;/code&gt;, then spelling out how we want to update it. The syntax is the same as what we used above with the LHS being the dependent variable and the RHS being the predictor variables. So for instance, if we wanted to add total distance to the model, we would call&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using update() to update statistical models
total.distance.model &amp;lt;- update(random.intercept.model, .~. + odometer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Some predictor variables are on very different scales: consider
## rescaling&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll come back to that error in a second. First, in the above code we tell R we want to update the random.intercept.model by adding odometer to the fixed effects. We also tell the function to retain the already-established variables with &lt;code&gt;.~.&lt;/code&gt;. Alternatively, if you wanted to start fresh on the predictor side of things, you could call &lt;code&gt;update(random.intercept.model, .~ odometer + (1|season/athlete))&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now that we’ve created our shiny new total.distance.model, we want to determine if adding total distance as a fixed effect statistically improved the model’s fit. This can be done in R with the &lt;code&gt;anova()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compare models with the anova() command
anova(random.intercept.model, total.distance.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: rpe.data
## Models:
## random.intercept.model: rpetl ~ (1 | season/athlete)
## total.distance.model: rpetl ~ (1 | season/athlete) + odometer
##                        Df   AIC   BIC logLik deviance  Chisq Chi Df
## random.intercept.model  4 36342 36366 -18167    36334              
## total.distance.model    5 34539 34569 -17265    34529 1805.3      1
##                        Pr(&amp;gt;Chisq)    
## random.intercept.model               
## total.distance.model    &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are four main things you can examine in this output: 1) the AIC, 2) the BIC, 3) the log-likelihood, and 4) the p-value for the chi-squared test comparing the log-likelihood of the two models. AIC, BIC, and logLik help us understand how well the models fit the data; values closer to zero are better. The change in logLik allows us determine if the change in the model fit is statistically significant (see &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Field_2012&#34;&gt;3&lt;/a&gt;]&lt;/span&gt; if you’re interested in the math), although there are two important caveats. 1) the models must be fit with maximum likelihood, not restricted maximum likelihood (REML = FALSE), and 2) subsequent models should be nested. That is, later models should contain all the variables from previous models (this will be clearer in a second). We can see from the above output that inclusion of total distance as a fixed effect statistically improved the model’s fit of the data with a p-value of 2 x 10^(-16). Let’s go ahead and add work rate, hi-speed running distance, time spent in drills, and total practice duration to the model and compare each step along the way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.1 &amp;lt;- update(total.distance.model, .~. + work.rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Some predictor variables are on very different scales: consider
## rescaling&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.2 &amp;lt;- update(model.1, .~. + hi.run)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Some predictor variables are on very different scales: consider
## rescaling&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.3 &amp;lt;- update(model.2, .~. + field.minutes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Some predictor variables are on very different scales: consider
## rescaling&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.4 &amp;lt;- update(model.3, .~. + duration)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Some predictor variables are on very different scales: consider
## rescaling&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(total.distance.model, model.1, model.2, model.3, model.4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: rpe.data
## Models:
## total.distance.model: rpetl ~ (1 | season/athlete) + odometer
## model.1: rpetl ~ (1 | season/athlete) + odometer + work.rate
## model.2: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run
## model.3: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run + 
## model.3:     field.minutes
## model.4: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run + 
## model.4:     field.minutes + duration
##                      Df   AIC   BIC logLik deviance   Chisq Chi Df
## total.distance.model  5 34539 34569 -17265    34529               
## model.1               6 34456 34492 -17222    34444  84.818      1
## model.2               7 34397 34439 -17192    34383  61.045      1
## model.3               8 34365 34413 -17175    34349  33.916      1
## model.4               9 33701 33755 -16842    33683 665.894      1
##                      Pr(&amp;gt;Chisq)    
## total.distance.model               
## model.1               &amp;lt; 2.2e-16 ***
## model.2               5.578e-15 ***
## model.3               5.756e-09 ***
## model.4               &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each subsequent model is a statistically significant improvement over the previous. Let’s further add season and phase within season as fixed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.5 &amp;lt;- update(model.4, .~. + season)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Some predictor variables are on very different scales: consider
## rescaling&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.6 &amp;lt;- update(model.5, .~. + phase)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Some predictor variables are on very different scales: consider
## rescaling&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(model.4, model.5, model.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: rpe.data
## Models:
## model.4: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run + 
## model.4:     field.minutes + duration
## model.5: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run + 
## model.5:     field.minutes + duration + season
## model.6: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run + 
## model.6:     field.minutes + duration + season + phase
##         Df   AIC   BIC logLik deviance    Chisq Chi Df Pr(&amp;gt;Chisq)    
## model.4  9 33701 33755 -16842    33683                               
## model.5 12 33699 33771 -16838    33675   8.1387      3    0.04323 *  
## model.6 15 33358 33448 -16664    33328 346.9856      3    &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, inclusion of season and phase statistically improved the model’s fit. We could continue to make our model more complicated by modeling interactions and different random effects structures, but I don’t want to get too far into the weeds today.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;centering-the-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Centering the Data&lt;/h4&gt;
&lt;p&gt;Instead, I want to draw your attention to the error we kept receiving each time a new model was created: “Warning: Some predictor variables are on very different scales: Consider rescaling.” Let’s print a summary of model.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by maximum likelihood  [&amp;#39;lmerMod&amp;#39;]
## Formula: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run +  
##     field.minutes + duration + season + phase
##    Data: rpe.data
## 
##      AIC      BIC   logLik deviance df.resid 
##  33358.3  33448.1 -16664.2  33328.3     2915 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.9356 -0.6017 -0.0475  0.4800  5.7434 
## 
## Random effects:
##  Groups         Name        Variance Std.Dev.
##  athlete:season (Intercept)  956     30.92   
##  season         (Intercept)    0      0.00   
##  Residual                   4796     69.25   
## Number of obs: 2930, groups:  athlete:season, 103; season, 4
## 
## Fixed effects:
##                   Estimate Std. Error t value
## (Intercept)     -1.140e+02  1.622e+01  -7.028
## odometer         2.841e-02  4.529e-03   6.274
## work.rate        2.776e-01  2.037e-01   1.362
## hi.run           4.588e-02  6.904e-03   6.645
## field.minutes   -8.601e-01  2.943e-01  -2.922
## duration         3.348e+00  1.375e-01  24.341
## seasonFall 2015 -1.998e+01  9.113e+00  -2.193
## seasonFall 2016 -1.090e+01  9.876e+00  -1.104
## seasonFall 2017 -3.363e+01  1.051e+01  -3.200
## phaseNC          1.211e+01  3.397e+00   3.564
## phasePOST        3.776e+00  4.262e+00   0.886
## phasePRE         8.011e+01  4.549e+00  17.609
## 
## Correlation of Fixed Effects:
##             (Intr) odomtr wrk.rt hi.run fld.mn duratn sF2015 sF2016 sF2017
## odometer     0.704                                                        
## work.rate   -0.815 -0.892                                                 
## hi.run       0.010 -0.417  0.127                                          
## field.mints -0.683 -0.823  0.791  0.197                                   
## duration    -0.305 -0.164  0.133 -0.052 -0.238                            
## sesnFll2015 -0.311  0.023 -0.005 -0.047 -0.034  0.044                     
## sesnFll2016 -0.310  0.033 -0.023 -0.029 -0.032  0.098  0.510              
## sesnFll2017 -0.309  0.025  0.004 -0.063  0.000  0.048  0.479  0.452       
## phaseNC      0.053  0.046 -0.074 -0.055 -0.014 -0.210 -0.017 -0.084 -0.045
## phasePOST    0.001  0.064 -0.069 -0.149  0.026 -0.132 -0.056 -0.055 -0.037
## phasePRE     0.117  0.118 -0.104 -0.111 -0.073 -0.249 -0.007 -0.068 -0.039
##             phasNC phPOST
## odometer                 
## work.rate                
## hi.run                   
## field.mints              
## duration                 
## sesnFll2015              
## sesnFll2016              
## sesnFll2017              
## phaseNC                  
## phasePOST    0.475       
## phasePRE     0.522  0.367
## fit warnings:
## Some predictor variables are on very different scales: consider rescaling
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From reading &lt;a href=&#34;https://rpubs.com/jimsavage/scale_issues&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; it seems this error can happen for a few different reasons, including fixed effects that are magnitudes different from one another and model coefficients that are magnitudes different from one another. If you look at the data and the model output, check and check. It would be a good idea to center our variables then. For a discussion on centering, see &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Field_2012&#34;&gt;3&lt;/a&gt;]&lt;/span&gt;; to see it in practice, see &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Carey_2016&#34;&gt;2&lt;/a&gt;]&lt;/span&gt;. I’m going to use grand mean centering (centering on pooled data) of the fixed effects similar to R1 from &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Carey_2016&#34;&gt;2&lt;/a&gt;]&lt;/span&gt;. I do want to preserve the fact that we’re examining data across multiple seasons, so I’ll be including &lt;code&gt;by = season&lt;/code&gt; in the data centering call below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpe.scaled &amp;lt;- rpe.data

# Create scaled versions of each of the fixed effects
# as.vector(scale()) is required as scale() normally returns a matrix
rpe.scaled[, c(&amp;quot;field.minutes&amp;quot;, &amp;quot;odometer&amp;quot;, &amp;quot;work.rate&amp;quot;, &amp;quot;hi.run&amp;quot;, &amp;quot;duration&amp;quot;) := 
             lapply(.SD, function(x) as.vector(scale(x))), 
           .SDcols = field.minutes:duration, by = season]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Grand mean centering alters the model coefficients, but it doesn’t actually change the interpretation of the model. So we’ll skip the rigamarole of building each step of the model in favor of only creating the scaled version of model.6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled.model.6 &amp;lt;- lmer(rpetl ~ (1|season/athlete) + odometer + work.rate + hi.run + 
                         field.minutes + duration + season + phase, 
                       data = rpe.scaled, REML = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(scaled.model.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by maximum likelihood  [&amp;#39;lmerMod&amp;#39;]
## Formula: rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run +  
##     field.minutes + duration + season + phase
##    Data: rpe.scaled
## 
##      AIC      BIC   logLik deviance df.resid 
##  33418.0  33507.7 -16694.0  33388.0     2915 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.9205 -0.6155 -0.0426  0.4741  5.5493 
## 
## Random effects:
##  Groups         Name        Variance Std.Dev.
##  athlete:season (Intercept)  964.1   31.05   
##  season         (Intercept)    0.0    0.00   
##  Residual                   4896.3   69.97   
## Number of obs: 2930, groups:  athlete:season, 103; season, 4
## 
## Fixed effects:
##                 Estimate Std. Error t value
## (Intercept)      232.516      7.079  32.845
## odometer          37.013      5.932   6.239
## work.rate          6.486      4.298   1.509
## hi.run            14.987      2.307   6.496
## field.minutes    -11.296      3.933  -2.872
## duration          52.126      2.189  23.813
## seasonFall 2015  -33.928      9.143  -3.711
## seasonFall 2016  -77.060      9.831  -7.838
## seasonFall 2017  -87.543     10.488  -8.347
## phaseNC           13.570      3.432   3.954
## phasePOST          4.376      4.317   1.014
## phasePRE          81.014      4.597  17.624
## 
## Correlation of Fixed Effects:
##             (Intr) odomtr wrk.rt hi.run fld.mn duratn sF2015 sF2016 sF2017
## odometer    -0.037                                                        
## work.rate    0.036 -0.886                                                 
## hi.run       0.034 -0.421  0.121                                          
## field.mints  0.024 -0.816  0.782  0.194                                   
## duration     0.075 -0.157  0.123 -0.047 -0.257                            
## sesnFll2015 -0.702  0.000 -0.004  0.008 -0.007  0.002                     
## sesnFll2016 -0.641 -0.015  0.002  0.026  0.005  0.018  0.511              
## sesnFll2017 -0.609  0.001  0.002  0.006 -0.006  0.003  0.479  0.445       
## phaseNC     -0.278  0.062 -0.091 -0.060 -0.027 -0.210 -0.008 -0.049 -0.019
## phasePOST   -0.178  0.080 -0.083 -0.162  0.015 -0.129 -0.055 -0.046 -0.037
## phasePRE    -0.225  0.122 -0.106 -0.114 -0.079 -0.242 -0.001 -0.034 -0.013
##             phasNC phPOST
## odometer                 
## work.rate                
## hi.run                   
## field.mints              
## duration                 
## sesnFll2015              
## sesnFll2016              
## sesnFll2017              
## phaseNC                  
## phasePOST    0.475       
## phasePRE     0.522  0.366
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lions-and-tigers-and-heteroscedasticity-oh-my&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Lions and Tigers and Heteroscedasticity, Oh My!&lt;/h4&gt;
&lt;p&gt;So how well does our model fit the data? Let’s plot the residuals to find out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(scaled.model.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(lm(predict(scaled.model.6, rpe.scaled) ~ rpe.scaled$rpetl))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-15-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-15-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-15-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Holy heteroscedasticity! That’s not what you want to see. If you’re unfamiliar, heteroscedasticity is more than just that one word your stats professor could never pronounce. One of the assumptions of linear models is that the model variance is consistent across all levels of the predictors. The fan-shaped residuals suggest this isn’t the case. Instead, as the predictors increase, the error in the prediction increases (variability in the athletes’ responses increases). That’s less than ideal when we’re attempting to predict rpetl. On the bright side, though, the Q-Q plot suggests the residuals are &lt;em&gt;relatively&lt;/em&gt; normal.&lt;/p&gt;
&lt;p&gt;Given we’re dealing with heteroscedastic–but relatively normal–residuals, one way to correct heteroscedasticity in our model is transformation of the dependent variable. Since rpetl is positively skewed, our main options are log transformation and square root transformation. Both tend to be useful for “pulling in” the tail of a skewed distribution, but log transformation is the more common of the two in sports science research. &lt;a href=&#34;http://www.sportsci.org/resource/stats/logtrans.html&#34; target=&#34;_blank&#34;&gt;Hopkins&lt;/a&gt; goes into some detail as to why if you’re interested. Also, there’s some discussion on log transformation in chapter 5 of &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Field_2012&#34;&gt;3&lt;/a&gt;]&lt;/span&gt;. Let’s add natural logged rpetl to the rpe.scaled data frame then re-run model 6. We’ll call this model final.model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpe.scaled[, log.rpetl := log(rpetl)]

final.model &amp;lt;- update(scaled.model.6, log.rpetl ~.)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(final.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by maximum likelihood  [&amp;#39;lmerMod&amp;#39;]
## Formula: 
## log.rpetl ~ (1 | season/athlete) + odometer + work.rate + hi.run +  
##     field.minutes + duration + season + phase
##    Data: rpe.scaled
## 
##      AIC      BIC   logLik deviance df.resid 
##   2460.7   2550.4  -1215.3   2430.7     2915 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.9599 -0.6227  0.0843  0.6488  3.9842 
## 
## Random effects:
##  Groups         Name        Variance Std.Dev.
##  athlete:season (Intercept) 0.02931  0.1712  
##  season         (Intercept) 0.00000  0.0000  
##  Residual                   0.12566  0.3545  
## Number of obs: 2930, groups:  athlete:season, 103; season, 4
## 
## Fixed effects:
##                 Estimate Std. Error t value
## (Intercept)      5.29222    0.03815 138.709
## odometer         0.04029    0.03012   1.338
## work.rate        0.20622    0.02180   9.459
## hi.run           0.03150    0.01170   2.691
## field.minutes    0.06492    0.02000   3.246
## duration         0.31320    0.01110  28.217
## seasonFall 2015 -0.17880    0.04960  -3.605
## seasonFall 2016 -0.40376    0.05327  -7.579
## seasonFall 2017 -0.47707    0.05711  -8.354
## phaseNC          0.07035    0.01740   4.043
## phasePOST        0.02321    0.02188   1.060
## phasePRE         0.32283    0.02331  13.848
## 
## Correlation of Fixed Effects:
##             (Intr) odomtr wrk.rt hi.run fld.mn duratn sF2015 sF2016 sF2017
## odometer    -0.035                                                        
## work.rate    0.033 -0.886                                                 
## hi.run       0.032 -0.420  0.120                                          
## field.mints  0.022 -0.816  0.782  0.193                                   
## duration     0.071 -0.155  0.122 -0.047 -0.257                            
## sesnFll2015 -0.706  0.001 -0.004  0.007 -0.007  0.002                     
## sesnFll2016 -0.646 -0.014  0.001  0.026  0.005  0.018  0.510              
## sesnFll2017 -0.610  0.001  0.002  0.005 -0.005  0.003  0.476  0.443       
## phaseNC     -0.261  0.062 -0.090 -0.060 -0.026 -0.210 -0.007 -0.047 -0.018
## phasePOST   -0.167  0.079 -0.083 -0.162  0.016 -0.129 -0.052 -0.043 -0.035
## phasePRE    -0.212  0.122 -0.107 -0.114 -0.078 -0.242  0.000 -0.032 -0.012
##             phasNC phPOST
## odometer                 
## work.rate                
## hi.run                   
## field.mints              
## duration                 
## sesnFll2015              
## sesnFll2016              
## sesnFll2017              
## phaseNC                  
## phasePOST    0.474       
## phasePRE     0.522  0.366
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(final.model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(lm(predict(final.model, rpe.scaled) ~ rpe.scaled$log.rpetl))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-16-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-16-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2018-11-05-r-individual-monitoring-part-2/index_files/figure-html/unnamed-chunk-16-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While still not perfect, log transforming rpetl has improved our heteroscedasticity situation. I have a feeling there’s an interaction missing from the fixed effects, but I’d rather not overcomplicate things today.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrapping Up&lt;/h3&gt;
&lt;p&gt;Now that we have our predictive model, the next step will be using the model to predict rpetl values and (hopefully) identify anomalous responses. Because this post is already super long, I’ve moved the predictive portion into a standalone post. But never fear, I’ve already written &lt;a href=&#34;/post/r-individual-monitoring-part-3/&#34; target=&#34;_blank&#34;&gt;part 3&lt;/a&gt;, so there won’t be a three month hiatius between posts! Once you’ve recovered from part 2, head on over to part 3 for the epic conclusion…or something.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Bartlett_2017&#34;&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;em&gt;Bartlett JD, O’Connor F, Pitchford N, Torres-Ronda L, Robertson SJ. &lt;/em&gt;Relationships between internal and external training load in team-sport athletes: Evidence for an individualized approach. International Journal of Sports Physiology and Performance 2017; 12: 230–234 Available from: &lt;a href=&#34;https://doi.org/10.1123/ijspp.2015-0791&#34;&gt;https://doi.org/10.1123/ijspp.2015-0791&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Carey_2016&#34;&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;Carey DL, Ong K, Morris ME, Crow J, Crossley KM. &lt;/em&gt;Predicting ratings of perceived exertion in australian football players: Methods for live estimation. International Journal of Computer Science in Sport 2016; 15: 64–77 Available from: &lt;a href=&#34;https://doi.org/10.1515/ijcss-2016-0005&#34;&gt;https://doi.org/10.1515/ijcss-2016-0005&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Field_2012&#34;&gt;
&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;em&gt;Field A, Miles J, Field Z. &lt;/em&gt;Discovering statistics using r. Sage Publishing, 2012&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Gelman_2006&#34;&gt;
&lt;p&gt;&lt;sup&gt;4&lt;/sup&gt; &lt;em&gt;Gelman A, Hill J. &lt;/em&gt;Data analysis using regression and multilevel/hierarchical models. Cambridge University Press, 2006 Available from: &lt;a href=&#34;https://doi.org/10.1017/cbo9780511790942&#34;&gt;https://doi.org/10.1017/cbo9780511790942&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Vandewiele_2017&#34;&gt;
&lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; &lt;em&gt;Vandewiele G, Geurkink Y, Lievens M, Ongenae F, Turck FD, Boone J. &lt;/em&gt;Enabling training personalization by predicting the session rate of perceived exertion (sRPE). 2017;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Ward_2018&#34;&gt;
&lt;p&gt;&lt;sup&gt;6&lt;/sup&gt; &lt;em&gt;Ward P, Coutts AJ, Pruna R, McCall A. &lt;/em&gt;Putting the “i” back in team. International Journal of Sports Physiology and Performance 2018; 1–14 Available from: &lt;a href=&#34;https://doi.org/10.1123/ijspp.2018-0154&#34;&gt;https://doi.org/10.1123/ijspp.2018-0154&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Speed play: Guiding skill through a seamlessly sequenced sprint curriculum</title>
      <link>/publication/speed-play/speed-play/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/speed-play/speed-play/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cluster set loading in the back squat: Kinetic and kinematic implications</title>
      <link>/publication/back-squat-cluster-kinematic-kinetic/back-squat-cluster-kinematic-kinetic/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/back-squat-cluster-kinematic-kinetic/back-squat-cluster-kinematic-kinetic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Individualized Athlete Monitoring in R, Part 1</title>
      <link>/post/2018-08-15-r-individual-monitoring/r-individual-monitoring-part-1/</link>
      <pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-08-15-r-individual-monitoring/r-individual-monitoring-part-1/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-back-into-the-swing-of-things&#34;&gt;Getting Back into the Swing of Things&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statistical-process-control-spc&#34;&gt;Statistical Process Control (SPC)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#performing-spc-in-r&#34;&gt;Performing SPC in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-fancy&#34;&gt;Getting Fancy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapping-up&#34;&gt;Wrapping Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code-recap&#34;&gt;Code Recap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;getting-back-into-the-swing-of-things&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Back into the Swing of Things&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;bender.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, it’s been a hot minute since my last post (whoops!). I’ve been busy with bobsledders and job hunting this summer, but I’ve finally found a bit of time to get back to the ole blog now that bobsled’s summer training is winding down and I’m preparing for an intercontinental move (keep an eye on Twitter for related news).&lt;/p&gt;
&lt;p&gt;A ton of great papers have come out this summer focused on various aspects of athlete monitoring. One that I particularly enjoyed was “Putting the ‘I’ Back in Team” by Patrick Ward, Aaron Coutts, Ricard Pruna, and Alan McCall &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Ward_2018&#34;&gt;6&lt;/a&gt;]&lt;/span&gt;. They didn’t discuss anything groundbreaking, but it was nice to see statistical process control get a bit more love (for additional commentary on SPC, see Sands et al. &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Sands_2017&#34;&gt;5&lt;/a&gt;]&lt;/span&gt;) while dancing around the idea of anomaly detection via modeling. The latter has been particularly interesting to me for the last year or so, but I haven’t gotten past the “fiddling stage” in R until the last month. We’ll get deeper into SPC today and will touch on anomaly detection in Part 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-process-control-spc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical Process Control (SPC)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;quality-control.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Like many of the other statistical techniques used in sports science, SPC has been borrowed from the business analytics world, specifically manufacturing quality control. The underlying theory of SPC is pretty straightforward–all “processes” possess a certain amount of variation or noise. Some of this variation is idiosyncratic (inherent to the individual process being studied), whereas additional unexplained variation may be introduced through alterations in the process. Processes experiencing normal variation are said to be “in control” or “stable” while a process experiencing random/intermittent unexplained variation outside its norm is considered “out of control.” Perfectly clear? Great!&lt;/p&gt;
&lt;p&gt;…In reality, that definition probably didn’t make a lot of sense. Let’s try again with a very practical example. The last few years, I’ve used weighted squat jumps (SJ) as a fatigue monitoring tool. My rationale for using weighted SJ came from my thesis and from a handful of papers that have shown SJ height from flight time may be more sensitive to fatigue state than countermovement jump (CMJ) height from flight time &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Gathercole_2015&#34;&gt;1&lt;/a&gt;–&lt;a href=&#34;#ref-Sams_2014&#34;&gt;4&lt;/a&gt;]&lt;/span&gt;. Each athlete will have their own “normal”–that is, a day-to-day range they will stay within when under “normal” levels of fatigue and when no training adaptations have occurred (strength changes, etc.). This range represents the athlete’s typical variation in their jump performance and will be inherent to that athlete. Some athletes will have a very tight inter-day variation in jump height (&amp;lt; 1 cm), whereas others may have greater inter-day fluctuations (2 - 4 cm). When the athlete’s jump height is within their normal range, we can say with relative certainty that the &lt;em&gt;process&lt;/em&gt; of their SJ height is “in control.” In the case of using SJ height as a proxy measure of fatigue state, we can assert that the athlete is not experiencing undue levels of fatigue…Well, maybe. Don’t rely on a single metric to solve all of life’s problems. If, however, the athlete’s jump height falls below an arbitrarily set threshold (typically 1 - 2 SD below their average), they can be flagged for deeper analysis and potential intervention as their SJ height is now “out of control.” This suggests they may be experiencing elevated levels of fatigue.&lt;/p&gt;
&lt;div id=&#34;performing-spc-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Performing SPC in R&lt;/h3&gt;
&lt;p&gt;It’s always easier to understand this stuff with visuals. If you’d like to follow along, the data can be found &lt;a href=&#34;jump-data.csv&#34;&gt;here&lt;/a&gt;. Let’s see what we’re working with:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;jumpDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;athlete&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;jumpType&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;flightTime&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;session&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Benjamin Babatunde&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.60&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Benjamin Babatunde&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.61&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Joo Garand&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.62&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Joo Garand&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.62&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steve Galligan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steve Galligan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jungsoo Moua&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.58&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jungsoo Moua&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.58&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Naadir el-Mahdavi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.63&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Naadir el-Mahdavi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SJ&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.62&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BL&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-15-r-individual-monitoring/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are real flight time values measured on switch mats, although the athletes’ names have been randomized. The data come from 21 vertical jump testing sessions across a fall season and include 1 “baseline” testing session and 20 in-season testing sessions that were carried out ~ 4 hours before most of the season’s matches. Each athlete completed 2 - 3 trials at each testing session, and the two closest trials were stored in the monitoring database.&lt;/p&gt;
&lt;p&gt;First things first, we need to obtain session averages for each athlete. I know &lt;code&gt;dplyr&lt;/code&gt; tends to be the go-to for data manipulation, so I’ve included both &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;data.table&lt;/code&gt; (my personal favorite) versions of the code throughout the remainder of the post. One of these days, I’ll get around to discussing the usefulness of using &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;data.table&lt;/code&gt; together…one day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Importing the data
jump.data &amp;lt;- read.csv(&amp;#39;jump-data.csv&amp;#39;)

# We need to convert the dates from factor to date type
jump.data$jumpDate &amp;lt;- as.Date(jump.data$jumpDate)

# dplyr syntax
dplyr.jump.means &amp;lt;- jump.data %&amp;gt;% group_by(jumpDate, athlete) %&amp;gt;%
  summarise(avg_ft = mean(flightTime))

# data.table syntax
jump.data &amp;lt;- data.table(jump.data)

dt.jump.means &amp;lt;- jump.data[, .(avg_ft = mean(flightTime)), by = .(jumpDate, athlete)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In either case, we now have mean session values for each athlete:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;jumpDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;athlete&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;avg_ft&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Benjamin Babatunde&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.605&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Joo Garand&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.620&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steve Galligan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.570&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jungsoo Moua&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.580&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Naadir el-Mahdavi&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.625&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Brandon Troxel&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.540&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aqeel al-Hossain&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.640&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Darin Russell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.575&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aiden Jaeger&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.535&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Tony Cha&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.635&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-15-r-individual-monitoring/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We need a few more pieces of information to create control charts for our analysis–namely, each athlete’s overall mean flight time and the standard deviation of their flight times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dplyr syntax
dplyr.jump.means &amp;lt;- dplyr.jump.means %&amp;gt;% group_by(athlete) %&amp;gt;% 
  mutate(ind_mean = round(mean(avg_ft), 3),
         ind_sd = round(sd(avg_ft), 3))

# data.table syntax
dt.jump.means[, &amp;#39;:=&amp;#39; (ind_mean = round(mean(avg_ft), 3), 
                      ind_sd = round(sd(avg_ft), 3)), by = athlete]

knitr::kable(dt.jump.means[1:10,])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;jumpDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;athlete&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;avg_ft&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ind_mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ind_sd&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Benjamin Babatunde&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.605&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.613&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Joo Garand&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.620&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.618&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steve Galligan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.570&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.587&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.010&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jungsoo Moua&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.580&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.575&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.010&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Naadir el-Mahdavi&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.625&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.596&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Brandon Troxel&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.540&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.540&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aqeel al-Hossain&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.640&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.597&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Darin Russell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.575&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.538&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aiden Jaeger&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.535&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.549&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Tony Cha&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.635&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.616&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.020&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Once we have the athletes’ means and standard deviations, we can calculate their upper and lower control limits. In this example, I’m using limits of mean ± 1.5 SD:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dplyr syntax
dplyr.jump.means &amp;lt;- dplyr.jump.means %&amp;gt;% group_by(athlete) %&amp;gt;%
  mutate(upper_limit = round(ind_mean + 1.5 * ind_sd, 3),
         lower_limit = round(ind_mean - 1.5 * ind_sd, 3)) %&amp;gt;% ungroup()

# data.table syntax
dt.jump.means[, &amp;#39;:=&amp;#39; (upper_limit = round(ind_mean + 1.5 * ind_sd, 3),
                      lower_limit = round(ind_mean - 1.5 * ind_sd, 3)),
              by = athlete]

knitr::kable(dt.jump.means[1:10,])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;jumpDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;athlete&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;avg_ft&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ind_mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ind_sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;upper_limit&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lower_limit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Benjamin Babatunde&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.605&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.613&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.014&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.634&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.592&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Joo Garand&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.620&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.618&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.638&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.598&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steve Galligan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.570&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.587&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.602&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.572&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jungsoo Moua&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.580&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.575&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.590&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.560&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Naadir el-Mahdavi&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.625&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.596&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.020&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.626&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.566&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Brandon Troxel&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.540&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.540&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.019&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.568&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.512&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aqeel al-Hossain&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.640&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.597&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.017&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.622&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.572&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Darin Russell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.575&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.538&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.017&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.564&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.513&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aiden Jaeger&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.535&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.549&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.025&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.586&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.512&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2015-08-15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Tony Cha&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.635&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.616&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.020&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.646&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.586&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now we have everything we need to plot control charts for our athletes. To start, I’m only going to plot the data from a single athlete because things can get cluttered pretty quickly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot a single athlete&amp;#39;s data
ggplot(dt.jump.means[athlete == &amp;#39;Steve Galligan&amp;#39;], aes(x = jumpDate, y = avg_ft)) + 
  geom_point() + geom_line() + 
  geom_hline(aes(yintercept = ind_mean)) + 
  geom_hline(aes(yintercept = upper_limit), linetype = &amp;#39;dotdash&amp;#39;) +
  geom_hline(aes(yintercept = lower_limit), linetype = &amp;#39;dotdash&amp;#39;) + theme_bw() +
  labs(title = &amp;#39;Jumps Over Time&amp;#39;, x = &amp;#39;Testing Date&amp;#39;, y = &amp;#39;Flight Time (s)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-15-r-individual-monitoring/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have a very basic control chart worked out. Obviously, we’re examining the data retrospectively for an athlete we managed &lt;em&gt;relatively&lt;/em&gt; well so a majority of his flight time values fall within his control limits (the drop you see at the end was after Friday-Sunday double overtime games where he played 110 minutes each game). Examining the plot brings up an important issue with SPC, though: process shifts. If you take a look at the first few testing sessions of the season, there is a noticeable increase in his performance leading into September. From knowing this athlete, I can tell you he came in unfit and having performed zero strength training that summer so his increased jump performance was to be expected. It’s safe to assume that we’ve captured his fitness changes in our monitoring data, and these fitness changes have created a process shift in his vertical jump performance. Not accounting for this shift will artificially inflate the control limits, which could cause us to miss otherwise important decreases in his monitoring data.&lt;/p&gt;
&lt;p&gt;So how can we deal with process shifts? Your chosen solution will ultimately boil down to how much free time you have, but I’ll wager most of you reading don’t have a lot. One of the easiest solutions is to use a rolling window approach to calculate your individual means and standard deviations. Like most things in sports science, there are no rules for rolling window size, but I would say whatever your window size is for your chronic rolling window (if you’re using A:C calculations) is a good place to start. For us, that was a 28-day window. So let’s reacalculate our limits and plot the data for the final 28 days of the season:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dplyr syntax using &amp;quot;chaining&amp;quot;
dplyr.recent.jumps &amp;lt;- dplyr.jump.means %&amp;gt;% filter(jumpDate &amp;gt;= max(jumpDate) - 28) %&amp;gt;%
  group_by(athlete) %&amp;gt;% mutate(ind_mean = mean(avg_ft),
                               ind_sd = sd(avg_ft),
                               upper_limit = ind_mean + 1.5 * ind_sd,
                               lower_limit = ind_mean - 1.5 * ind_sd)

# data.table syntax using chaining; data.table functions a little differently
dt.recent.jumps &amp;lt;- dt.jump.means[jumpDate &amp;gt;= max(jumpDate) - 28]

dt.recent.jumps[, &amp;#39;:=&amp;#39; (ind_mean = mean(avg_ft),
                        ind_sd = sd(avg_ft)),
                by = athlete][, &amp;#39;:=&amp;#39; (upper_limit = ind_mean + 1.5 * ind_sd,
                                      lower_limit = ind_mean - 1.5 * ind_sd),
                              by = athlete]

ggplot(dt.recent.jumps[athlete == &amp;#39;Steve Galligan&amp;#39;], aes(x = jumpDate, y = avg_ft)) + 
  geom_point() + geom_line() + 
  geom_hline(aes(yintercept = ind_mean)) + 
  geom_hline(aes(yintercept = upper_limit), linetype = &amp;#39;dotdash&amp;#39;) +
  geom_hline(aes(yintercept = lower_limit), linetype = &amp;#39;dotdash&amp;#39;) + theme_bw() +
  labs(title = &amp;#39;Jumps Over Time&amp;#39;, x = &amp;#39;Testing Date&amp;#39;, y = &amp;#39;Flight Time (s)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-15-r-individual-monitoring/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In both cases, the final testing session of the season fell below the athlete’s control limits, so both roads would have led to Rome. While this approach worked in the current case, you may have to adjust your window based on the frequency of your testing. For the year in question, we tested every ~ 4 days, but the next season was closer to every 10 days. Implementing a 28-day rolling window for both seasons would provide drastically different numbers of sessions for our calculations. In the case of a college sport, an alternative may be removal of the pre-season from the calculations. Ultimately, though, this will be an individualized decision based on your specific testing and monitoring setup.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-fancy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting Fancy&lt;/h3&gt;
&lt;p&gt;There are no hard and fast rules for what threshold constitutes an out of control process. For instance, a threshold of 1 SD (.16 probability of occurring by random chance) may be too reactive to otherwise harmless fluctuations in performance while a threshold of 2 SD (.025 probability of occurring by random chance) may be too strict &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Sands_2017&#34;&gt;5&lt;/a&gt;,&lt;a href=&#34;#ref-Ward_2018&#34;&gt;6&lt;/a&gt;]&lt;/span&gt;. Personally, I prefer a threshold of 1.5 SD (.065 probability of occurring by random chance), but other methods such as setting an “alert” threshold and a “warning” threshold (e.g. 1 SD = alert and 1.5 or 2 SD = warning) or implementing more sophisticated rules (see &lt;a href=&#34;https://en.wikipedia.org/wiki/Control_chart#Rules_for_detecting_signals&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt; for a discussion on different rule sets) are also possible &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Ward_2018&#34;&gt;6&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example, let’s apply alert (1 SD) and warning (1.5 SD) thresholds to our data from the first example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dplyr syntax
dplyr.jump.means &amp;lt;- dplyr.jump.means %&amp;gt;% group_by(athlete) %&amp;gt;%
  rename(upper_limit_1.5SD = upper_limit, lower_limit_1.5SD = lower_limit) %&amp;gt;%
  mutate(upper_limit_1SD = round(ind_mean + ind_sd, 3),
         lower_limit_1SD = round(ind_mean - ind_sd, 3)) %&amp;gt;% ungroup()

# data.table syntax
setnames(dt.jump.means,
         old = c(&amp;#39;upper_limit&amp;#39;, &amp;#39;lower_limit&amp;#39;),
         new = c(&amp;#39;upper_limit_1.5SD&amp;#39;, &amp;#39;lower_limit_1.5SD&amp;#39;))

dt.jump.means[, &amp;#39;:=&amp;#39; (upper_limit_1SD = round(ind_mean + ind_sd, 3),
                      lower_limit_1SD = round(ind_mean - ind_sd, 3)
                      ), by = athlete]

ggplot(dt.jump.means[athlete == &amp;#39;Steve Galligan&amp;#39;], aes(x = jumpDate, y = avg_ft)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = ind_mean)) +
  geom_hline(aes(yintercept = lower_limit_1.5SD)) +
  geom_hline(aes(yintercept = upper_limit_1.5SD)) +
  geom_hline(aes(yintercept = lower_limit_1SD), linetype = &amp;#39;dashed&amp;#39;) +
  geom_hline(aes(yintercept = upper_limit_1SD), linetype = &amp;#39;dashed&amp;#39;) +
  theme_bw() +
  labs(title = &amp;#39;Jumps Over Time&amp;#39;, x = &amp;#39;Testing Date&amp;#39;, y = &amp;#39;Flight Time (s)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-15-r-individual-monitoring/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s also possible to color code the plots based on whether they violate your chosen thresholds or more advanced rule sets. The code gets very messy (I originally included example code but removed it because it was difficult to follow), so if you want to use color coding or more advanced rule sets, I would seriously recommend exploring one of the specially-built SPC packages available on CRAN. &lt;code&gt;qcc&lt;/code&gt;, for instance, can create both individual control charts (“I” charts) and EWMA-based charts. Selection on chart type will depend on the amount of data you have and the window size you’ve chosen.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;If you really want to go down the SPC rabbit hole, there are R packages dedicated to SPC analysis (&lt;code&gt;spc&lt;/code&gt;, &lt;code&gt;qicharts&lt;/code&gt;, &lt;code&gt;qcc&lt;/code&gt;). I’ve played around with &lt;code&gt;qcc&lt;/code&gt; a bit to create both “I” and EWMA control charts, but I don’t have enough experience with the package to provide any sort of guidance.&lt;/p&gt;
&lt;p&gt;I had originally planned on covering anomaly detection in this post as well, but I’m pretty sure all our brains would be mush by the end. Look for Part 2 to drop in the next few days! In the meantime, feel free to drop me a line on &lt;a href=&#34;https://twitter.com/DrMattSams&#34; target=&#34;_blank&#34;&gt;twitter&lt;/a&gt; or email me at &lt;a href=&#34;mailto:samsperformancetraining@gmail.com&#34;&gt;samsperformancetraining@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code-recap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code Recap&lt;/h2&gt;
&lt;p&gt;To recap the code used in Part 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Required packages
library(data.table)
library(ggplot2)
library(knitr)
library(dplyr)

# Importing the data
jump.data &amp;lt;- read.csv(&amp;#39;3-Individual_Monitoring/jump-data.csv&amp;#39;)

# We need to convert the dates from factor to date type
jump.data$jumpDate &amp;lt;- as.Date(jump.data$jumpDate)

# --- Session Averages --- #

# dplyr syntax
dplyr.jump.means &amp;lt;- jump.data %&amp;gt;% group_by(jumpDate, athlete) %&amp;gt;%
  summarise(avg_ft = mean(flightTime))

# data.table syntax
jump.data &amp;lt;- data.table(jump.data)

dt.jump.means &amp;lt;- jump.data[, .(avg_ft = mean(flightTime)), by = .(jumpDate, athlete)]

# --- Individual Averages and SD --- #
# dplyr syntax
dplyr.jump.means &amp;lt;- dplyr.jump.means %&amp;gt;% group_by(athlete) %&amp;gt;% 
  mutate(ind_mean = round(mean(avg_ft), 3),
         ind_sd = round(sd(avg_ft), 3))

# data.table syntax
dt.jump.means[, &amp;#39;:=&amp;#39; (ind_mean = round(mean(avg_ft), 3), 
                      ind_sd = round(sd(avg_ft), 3)), by = athlete]

# --- Plot 1 --- #
# Plot a single athlete&amp;#39;s data
ggplot(dt.jump.means[athlete == &amp;#39;Steve Galligan&amp;#39;], aes(x = jumpDate, y = avg_ft)) + 
  geom_point() + geom_line() + 
  geom_hline(aes(yintercept = ind_mean)) + 
  geom_hline(aes(yintercept = upper_limit), linetype = &amp;#39;dotdash&amp;#39;) +
  geom_hline(aes(yintercept = lower_limit), linetype = &amp;#39;dotdash&amp;#39;) + theme_bw() +
  labs(title = &amp;#39;Jumps Over Time&amp;#39;, x = &amp;#39;Testing Date&amp;#39;, y = &amp;#39;Flight Time (s)&amp;#39;)

# --- Rolling Window and Plot --- #
# dplyr syntax using &amp;quot;chaining&amp;quot;
dplyr.recent.jumps &amp;lt;- dplyr.jump.means %&amp;gt;% filter(jumpDate &amp;gt;= max(jumpDate) - 28) %&amp;gt;%
  group_by(athlete) %&amp;gt;% mutate(ind_mean = mean(avg_ft),
                               ind_sd = sd(avg_ft),
                               upper_limit = ind_mean + 1.5 * ind_sd,
                               lower_limit = ind_mean - 1.5 * ind_sd)

# data.table syntax using chaining; data.table functions a little differently
dt.recent.jumps &amp;lt;- dt.jump.means[jumpDate &amp;gt;= max(jumpDate) - 28]

dt.recent.jumps[, &amp;#39;:=&amp;#39; (ind_mean = mean(avg_ft),
                        ind_sd = sd(avg_ft)),
                by = athlete][, &amp;#39;:=&amp;#39; (upper_limit = ind_mean + 1.5 * ind_sd,
                                      lower_limit = ind_mean - 1.5 * ind_sd),
                              by = athlete]

ggplot(dt.recent.jumps[athlete == &amp;#39;Steve Galligan&amp;#39;], aes(x = jumpDate, y = avg_ft)) + 
  geom_point() + geom_line() + 
  geom_hline(aes(yintercept = ind_mean)) + 
  geom_hline(aes(yintercept = upper_limit), linetype = &amp;#39;dotdash&amp;#39;) +
  geom_hline(aes(yintercept = lower_limit), linetype = &amp;#39;dotdash&amp;#39;) + theme_bw() +
  labs(title = &amp;#39;Jumps Over Time&amp;#39;, x = &amp;#39;Testing Date&amp;#39;, y = &amp;#39;Flight Time (s)&amp;#39;)

# --- Having both alert and warning thresholds --- #
# dplyr syntax
dplyr.jump.means &amp;lt;- dplyr.jump.means %&amp;gt;% group_by(athlete) %&amp;gt;%
  rename(upper_limit_1.5SD = upper_limit, lower_limit_1.5SD = lower_limit) %&amp;gt;%
  mutate(upper_limit_1SD = round(ind_mean + ind_sd, 3),
         lower_limit_1SD = round(ind_mean - ind_sd, 3)) %&amp;gt;% ungroup()

# data.table syntax
setnames(dt.jump.means,
         old = c(&amp;#39;upper_limit&amp;#39;, &amp;#39;lower_limit&amp;#39;),
         new = c(&amp;#39;upper_limit_1.5SD&amp;#39;, &amp;#39;lower_limit_1.5SD&amp;#39;))

dt.jump.means[, &amp;#39;:=&amp;#39; (upper_limit_1SD = round(ind_mean + ind_sd, 3),
                      lower_limit_1SD = round(ind_mean - ind_sd, 3)
                      ), by = athlete]

ggplot(dt.jump.means[athlete == &amp;#39;Steve Galligan&amp;#39;], aes(x = jumpDate, y = avg_ft)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = ind_mean)) +
  geom_hline(aes(yintercept = lower_limit_1.5SD)) +
  geom_hline(aes(yintercept = upper_limit_1.5SD)) +
  geom_hline(aes(yintercept = lower_limit_1SD), linetype = &amp;#39;dashed&amp;#39;) +
  geom_hline(aes(yintercept = upper_limit_1SD), linetype = &amp;#39;dashed&amp;#39;) +
  theme_bw() +
  labs(title = &amp;#39;Jumps Over Time&amp;#39;, x = &amp;#39;Testing Date&amp;#39;, y = &amp;#39;Flight Time (s)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Gathercole_2015&#34;&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;em&gt;Gathercole RJ, Sporer BC, Stellingwerff T, Sleivert GG. &lt;/em&gt;Comparison of the capacity of different jump and sprint field tests to detect neuromuscular fatigue. Journal of Strength and Conditioning Research 2015; 29: 2522–2531 Available from: &lt;a href=&#34;https://doi.org/10.1519/jsc.0000000000000912&#34;&gt;https://doi.org/10.1519/jsc.0000000000000912&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Hoffman_2002&#34;&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;Hoffman JR, Haresh CM, Newton RU, Rubin MR, French DN, Volek JS, Sutherland J, Robertson M, Gomez AL, Ratamess NA, Kang J, Kraemer WJ. &lt;/em&gt;Performance, biochemical, and endocrine changes during a competitive football game. Medicine and Science in Sports and Exercise 2002; 34: 1845–1853 Available from: &lt;a href=&#34;https://doi.org/10.1249/01.MSS.0000035373.26840.F8&#34;&gt;https://doi.org/10.1249/01.MSS.0000035373.26840.F8&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hortobgyi1991voluntary&#34;&gt;
&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;em&gt;Hortob’agyi T, Lambert NJ, Kroll WP. &lt;/em&gt;Voluntary and reflex responses to fatigue with stretch-shortening exercise. Canadian journal of sport sciences = Journal canadien des sciences du sport 1991; 16: 142–150 Available from: &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/1647860&#34;&gt;http://www.ncbi.nlm.nih.gov/pubmed/1647860&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sams_2014&#34;&gt;
&lt;p&gt;&lt;sup&gt;4&lt;/sup&gt; &lt;em&gt;Sams ML. &lt;/em&gt;Comparison of static and countermovement jump variables in relation to estimated training load and subjective measures of fatigue. 2014;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sands_2017&#34;&gt;
&lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; &lt;em&gt;Sands WA, Kavanaugh AA, Murray SR, McNeal JR, Jemni M. &lt;/em&gt;Modern techniques and technologies applied to training and performance monitoring. International Journal of Sports Physiology and Performance 2017; 12: S2–63–S2–72 Available from: &lt;a href=&#34;https://doi.org/10.1123/ijspp.2016-0405&#34;&gt;https://doi.org/10.1123/ijspp.2016-0405&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Ward_2018&#34;&gt;
&lt;p&gt;&lt;sup&gt;6&lt;/sup&gt; &lt;em&gt;Ward P, Coutts AJ, Pruna R, McCall A. &lt;/em&gt;Putting the “i” back in team. International Journal of Sports Physiology and Performance 2018; 1–14 Available from: &lt;a href=&#34;https://doi.org/10.1123/ijspp.2018-0154&#34;&gt;https://doi.org/10.1123/ijspp.2018-0154&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Repetition-to-repetition differences using cluster and accentuated eccentric loading in the back squat</title>
      <link>/publication/back-squat-cluster-ael-repetition-differences/back-squat-cluster-ael-repetition-differences/</link>
      <pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/back-squat-cluster-ael-repetition-differences/back-squat-cluster-ael-repetition-differences/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>S3 2018 Recap</title>
      <link>/post/2018-06-24-s3-2018-recap/s3-2018-recap/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-06-24-s3-2018-recap/s3-2018-recap/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#an-overview&#34;&gt;An Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recap&#34;&gt;Recap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-personal-thoughts&#34;&gt;Some Personal Thoughts&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#on-evolution-no-not-that-one&#34;&gt;On Evolution (No, Not That One)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#on-the-sports-science-see-saw&#34;&gt;On The Sports Science See-Saw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#on-machine-learning&#34;&gt;On Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#staying-out-of-the-weeds&#34;&gt;Staying out of the Weeds&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;an-overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Overview&lt;/h2&gt;
&lt;p&gt;The first S3 &lt;a href=&#34;https://twitter.com/S3Conference&#34; target=&#34;_blank&#34;&gt;(Sports Science Summit)&lt;/a&gt; is officially in the books. Organized (at least from what I can tell) and moderated by Gary McCoy, the conference was meant to provide insights from some of the most progressive sports science programs in the US. I have to be honest when I say I was skeptical when I first heard about the conference last week, because I’ve been burned more than once by larger national conferences that promised a big game yet failed to deliver. Let me just say, S3 delivered! Each of the talks and panel discussions was extremely interesting and gave me a little more hope for sports science in the US (I’ll save my rant for another day).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;p&gt;Gary McCoy started things off with some opening remarks and his beliefs on the current state of sports science in the US. It didn’t take long for me to decide we’re kindred spirits, because one of the first points he made was that sports science isn’t a “thing”–it’s a process. Too often, coaching staffs hire a “sports scientist” with the hope that they’ll solve all the team’s problems overnight. When things haven’t changed appreciably in the first few months (or even the first year), said sports scientist should probably start updating their resume. Further, cue the following quote from the coach/admin: “Yeah, we tried that sports science thing, but it didn’t really work for us.” These all-too-common scenarios tend to pop up for two reasons I think: 1) Some individuals and companies tend to oversell the benefits of a sports science program for their own short-term gain while conflating the presence of technology with sports science and 2) there is a disconnect between what we do and what administrators think we do.&lt;/p&gt;
&lt;p&gt;To the first point, I’ve spoken to several coaches who have purchased X hardware and software because a company rep promised them the world (eliminate injuries, win every game, assess player readiness with &lt;strong&gt;this one weird metric&lt;/strong&gt;, that sorta stuff). Tens of thousands of dollars (and one or two added employees) later, that shiny hardware sits on the shelf collecting dust and the password to the software platform has long been forgotten. The coach is left with a bad taste in their mouth about “that sports science thing,” and progress in the field is set back.&lt;/p&gt;
&lt;p&gt;(I spent way too long looking for a “What I think I do” meme. Imagine there’s one here for this next bit.)&lt;/p&gt;
&lt;p&gt;The above situation tends to play out over and over again because there’s a disconnect between what sports science actually is and what those outside the field think it is. At its core, sports science is about optimizing performance, maintaining or improving athlete wellbeing, and minimizing injury risk. That’s it. Sports science isn’t running logistic regression models, making pretty graphics to impress stakeholders, and telling the coach and S&amp;amp;C staff “no” or “do less.” Instead, sports science is looking at an athlete or a team and attempting to answer questions. E.g.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Why do we have so many injuries at this time of year?”&lt;/li&gt;
&lt;li&gt;“How can we increase our player availability?”&lt;/li&gt;
&lt;li&gt;“How fit do we need to be?”&lt;/li&gt;
&lt;li&gt;“Can we train through this opponent?”&lt;/li&gt;
&lt;li&gt;“How can we make practice more efficient?”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;…With each question ultimately referring back to those goals I mentioned before. We might use a healthy dose of statistics and data visualization techniques along the way to answering said questions–and we might even have to have tough conversations with a coach from time to time–but we use data and the tools available to us as a means to an end, not ends in themselves. And unfortunately, the process of identifying a question, collecting and analyzing data, implementing a plan, and re-assessing takes time. I spent five years with ETSU men’s soccer, and while some questions were answered relatively quickly (“How can we improve player availability during conference play?”), questions like “How do we reduce the likelihood of an ACL injury after spring break?” were tougher nuts to crack (4 years of poring over data in fact). In the current age of smartphones, text messages, and immediate gratification, explaining to a coach or athletic director that it might take a while to effect positive change isn’t very well received. Yet it’s imperative we help administrators understand sports science is a process that takes time to bring about change, not some nebulous thing that works or doesn’t work.&lt;/p&gt;
&lt;p&gt;That thought ties in nicely with a common theme shared by many of the speakers (&lt;a href=&#34;https://twitter.com/StrengthCoach21&#34; target=&#34;_blank&#34;&gt;Gary McCoy&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/HulsShaun&#34; target=&#34;_blank&#34;&gt;Shaun Huls&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/SteveTashjian&#34; target=&#34;_blank&#34;&gt;Steve Tashjian&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/Ben_J_Peterson&#34; target=&#34;_blank&#34;&gt;Ben Peterson&lt;/a&gt;, and the &lt;a href=&#34;https://twitter.com/DaveTenney/status/1008743782311972869&#34; target=&#34;_blank&#34;&gt;Orlando Magic high performance staff&lt;/a&gt;): communication reigns supreme in the high performance world. To have a successful high performance team, &lt;em&gt;everyone&lt;/em&gt; needs to be on the same page. That includes the coaches, S&amp;amp;C, sports medicine, sports science, sports nutrition, you name it. Further, we need to lean on and collaborate with each other instead of selfishly staying inside our little box and never reaching across the aisle. Obviously, we shouldn’t overstep into another professional’s domain (I’m not going to tell my head or assistant coach how to design a possession drill), but we should work synergistically with each member of the high performance team to provide the best service possible to our athletes (I would suggest drill dimensions, team sizes, drill lengths, etc. to the coaches to help us hit our desired internal and external load targets). I’ll again refer you back to the three goals of sports science I mentioned above.&lt;/p&gt;
&lt;p&gt;Ben Peterson had an especially interesting take on communication in the high performance world. Most of us are trained in hard skills during our masters and PhD programs, but few programs emphasize the soft skill side of things (etiquette, listening, getting along, smalltalk, that sort of stuff). Yet the soft skills are vital in overcoming that administrative disconnect I mentioned before. You naturally pick up some soft skills in the course of working with athletes and coaches from a variety of backgrounds, but Ben suggested that making a conscious effort to improve your soft skills can go a long way in improving coach and athlete buy-in and can ultimately make life easier for all parties involved. From personal experience, dealing with athletes from less rigid cultural backgrounds became a whole lot easier once I took a more empathetic and positive approach.&lt;/p&gt;
&lt;p&gt;The afternoon’s talks were my personal favorites. Full disclosure (if you couldn’t already guess), I’m a technology and monitoring nerd. And boy, did John Meyer and Marcus Elliot deliver in spades. John is the Associate AD of Sports Science and Performance at USC (the Californian variety), while Marcus is the founder and head of P3 (Peak Performance Project). Both have spent their respective careers collecting and analyzing vast amounts of monitoring data as a means to improve the training and rehabilitative processes. They shared some pretty interesting work on concussion management, return to play, and predictive analytics that had me turning to the guy next to me and saying, “Well, guess I’m moving to LA to work for one of these guys.” I’m not sure if I was kidding. Suffice to say, I plan on keeping an eye on the work they’re doing and have some new ideas I want to implement in my own athlete training and monitoring.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-personal-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some Personal Thoughts&lt;/h2&gt;
&lt;p&gt;While I really enjoyed the conference, I do have some disagreements with a few of the speakers on a philosophical level.&lt;/p&gt;
&lt;div id=&#34;on-evolution-no-not-that-one&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On Evolution (No, Not That One)&lt;/h3&gt;
&lt;p&gt;Several speakers mentioned the idea of constant evolution of your sports science program. While I agree that a program that isn’t pushing forward and seeking to continually improve itself year-after-year is effectively dead, evolution for evolution’s sake (or to be different or “brave” as one speaker put it) is just as damaging as stagnation. Yes, we should constantly look to improve our training program, monitoring program, coaching feedback, etc., but change should occur in a logical, progressive way that addresses one (or more) of our three goals (optimized performance, maintained or improved wellbeing, reduced injury risk). It’s OK that you’ve used the same vertical jump protocol to assess your athletes’ explosive capabilities for the last 8 years &lt;strong&gt;AS LONG AS&lt;/strong&gt; the data you obtain from the test are leveraged in some way. You might even improve your testing protocol by adding new technology that provides you deeper insights (Vertec -&amp;gt; switch mats -&amp;gt; force plates). And hey, it’s perfectly fine to test new things with your athletes as long as you can justify their inclusion and they won’t put an undue burden on the athletes. As an example, my first year with ETSU men’s soccer didn’t involve any athlete monitoring past sRPE and pre- and post-season lab testing. My second year, we used force plates to monitor the athletes’ vertical jumps once a week. We found that weighted squat jump height was strongly related to the athletes’ accumulated training load, so the next three years involved performing weighted squat jumps on match day to assess the athletes’ fatigue states. We used that data both to make training modifications in the moment and to adjust our training program in subsequent seasons. My point is that while the test was modified over time (force plates -&amp;gt; a switch mat, once weekly -&amp;gt; every pre-game), the fundamental core of using the data to monitor the athletes’ fatigue state remained the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;on-the-sports-science-see-saw&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On The Sports Science See-Saw&lt;/h3&gt;
&lt;p&gt;This is tangentially related to the previous point. One thing that continues to frustrate me about the field is that we tend to wildly swing from one viewpoint to another. In ACL rehabilitative circles, for instance, asymmetry was the only thing that mattered for years. Then, someone proclaimed asymmetry was dead and that force production was all that mattered. Naturally, everyone suddenly parroted that as gospel. In reality, &lt;em&gt;both&lt;/em&gt; are important. Yes, the involved limb needs to be able to tolerate the forces encountered in match play, but if the non-involved limb is experiencing disproportionately greater forces either from “picking up the slack” or a learned compensation pattern, there’s a good chance of a follow-up injury. In the words of my all-time favorite commercial,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;porque.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The same goes for nutrition and training theory. Some ideas were shared during the conference that came across as “60 years of performance research is wrong”…based on my observations…with a single group of athletes…over a two year span. All I’m saying is pump the brakes a bit, let the research develop, and take a more nuanced approach to what you’re publicizing. Of course, if what you’re doing is working for you, great, keep it up. But be careful about making broad generalizations to other populations before the data are in. Likewise, as a practitioner listening to these types of presentations, critically evaluate what you’re hearing and don’t be quick to chase every new fad just because it worked with one group of athletes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;on-machine-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On Machine Learning&lt;/h3&gt;
&lt;p&gt;Interestingly, most of the speakers didn’t feel machine learning would have a major impact on the field in the near future. Maybe I’m stepping on my previous point a bit here, but I have a feeling machine learning will have an impact sooner than we think. Don’t get me wrong, I don’t think machine learning is going to solve all of our problems, but I do believe we’ll be able to answer some narrowly-defined questions as technology continues to develop and as the data we collect on athletes continues to increase. For instance, several papers have come out over the last three years that have used supervised machine learning to predict sRPE values from external workload measures (distance traveled, minutes played, sprints performed, etc.).&lt;/p&gt;
&lt;p&gt;An interesting application that I’ve been toying around with is the idea of “anomaly detection.” If we predict a certain sRPE value for an athlete, yet their true response is substantially greater than the prediction (and the error, of course), we might flag that athlete for a deeper dive into their data. If their pre-training mood state and HRV are displaying negative trends, we might provide them some additional rest. Likewise, if everything looks good from a pre-training standpoint, maybe something happened in training that they’re trying to conceal (be it injury, a bad practice, getting yelled at by coach, whatever).&lt;/p&gt;
&lt;p&gt;Another application of machine learning I see becoming popular is dimension reduction and identification of individualized metrics for each athlete. We can already perform dimension reduction to some degree with correlation analyses, PCA, and that sort of stuff, but machine learning may be able to help us better understand what external workload variables drive a specific athlete’s internal response. See &lt;a href=&#34;http://dx.doi.org/10.1123/ijspp.2015-0791&#34; target=&#34;_blank&#34;&gt;Bartlett et al.&lt;/a&gt; as an example of what I’m talking about.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;staying-out-of-the-weeds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Staying out of the Weeds&lt;/h3&gt;
&lt;p&gt;Most importantly, though, I think it’s a good idea to echo what I said at the end of my last blog post: master the big before worrying about the small. The college athletes I’ve worked with for the last six years are very different compared to the Olympic athletes I’ve had the privilege to work with. The Olympic athletes make life easy (…well, most of the time): they sleep 9 - 10 hours a night, they take a 20-minute mid-day nap, they eat exactly what they need to eat when they need to eat it, and they’re extremely neurotic about their training. For them, extremely in-depth analysis of their data, experimentation with new training modalities, supervised machine learning to individualize what we’re monitoring, and all that good stuff are worthwhile endeavors. For my collegiate athletes…not so much. They don’t sleep, they take four hour naps because they didn’t sleep, they don’t eat, their time management skills are shit, and most of them are not invested in the training process. For those athletes, I’m wasting my time by digging super deep into their data. Instead of doing a deep dive to understand why they didn’t perform well in the last match, all I need to do is ask a teammate to find out they were up until 3 AM playing cards in the hotel lobby (unfortunately, a true story…we got spanked that match).&lt;/p&gt;
&lt;p&gt;So before you implement 15 different tools to monitor your athletes, start by reading the room. Understand the team and the team culture. If you’re dealing with immature athletes who aren’t that invested in the process, start small and use a few quick, non-invasive methods of assessment. Focus on the big areas (are they sleeping? eating?) and go from there. As you develop a better culture and get the athletes to buy into what you’re doing, you can begin to implement further monitoring techniques and maybe even get into some advanced methods of assessment.&lt;/p&gt;
&lt;p&gt;Anyway, that’s enough ranting for one blog post. I’d love to get your thoughts on what I discussed in this post, so feel free to reach out on Twitter &lt;a href=&#34;https://twitter.com/DrMattSams&#34; target=&#34;_blank&#34;&gt;(@DrMattSams)&lt;/a&gt; or shoot me an email: &lt;a href=&#34;mailto:samsperformancetraining@gmail.com&#34;&gt;samsperformancetraining@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accentuated eccentric loading and cluster set configurations in the back squat: A kinetic and kinematic analysis</title>
      <link>/publication/ael-kinematic-kinetic-analysis/ael-kinematic-kinetic-analysis/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/ael-kinematic-kinetic-analysis/ael-kinematic-kinetic-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Leveraging R to Calculate Acute:Chronic Workload Ratios</title>
      <link>/post/2018-06-09-r-acwr-intro/r-acwr-intro/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-06-09-r-acwr-intro/r-acwr-intro/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#acutechronic-workload-ratios&#34;&gt;Acute:Chronic Workload Ratios?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#an-extremely-hot-topic&#34;&gt;An Extremely Hot Topic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calculating-acwr-in-r&#34;&gt;Calculating ACWR in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-data&#34;&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-than-one-way-to-skin-a-cat&#34;&gt;More Than One Way to Skin a Cat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coupled-vs.-uncoupled&#34;&gt;Coupled vs. Uncoupled&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapping-up&#34;&gt;Wrapping Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code-recap&#34;&gt;Code Recap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;acutechronic-workload-ratios&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acute:Chronic Workload Ratios?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Confused-Gandalf.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you’ve been keeping up with the athlete workload monitoring world, you’ve probably heard of the concept of acute:chronic workload ratios (ACWR). In case you haven’t, the concept is relatively simple: how does the recently accumulated training load for an athlete (typically the last 3 - 7 days) compare to their chronic accumulation of training load (previous 3 - 6 weeks of training)? We can create a rolling average of the athlete’s acute and chronic workload values and then divide the acute value by the chronic value to calculate an ACWR. These workloads can come from both external (balls pitched, distance travelled, etc.) and internal (session RPE, heart rate TRIMP, etc.) measures of training load &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Ehrmann_2016&#34;&gt;2&lt;/a&gt;,&lt;a href=&#34;#ref-Hulin_2013&#34;&gt;4&lt;/a&gt;,&lt;a href=&#34;#ref-Hulin_2015&#34;&gt;5&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;ACWR values provide a very intuitive snapshot of the training stress being experienced by the athlete and can be used to describe the “preparedness” of the athlete–that is, the relationship between their “fitness” and accumulated fatigue (see &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-DeWeese_2015&#34;&gt;1&lt;/a&gt;]&lt;/span&gt;). Normal training produces both adaptations in the athlete’s fitness qualities and accumulated fatigue. Unfortunately, fatigue tends to mask complete expression of fitness qualities, so we want to avoid states of high fatigue when the athletes need to produce. Similarly, high levels of fatigue increase injury risk (although recent research &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Malone_2018&#34;&gt;7&lt;/a&gt;]&lt;/span&gt; has highlighted that the relationship is a bit more complicated than that), so we need to be cognizant of our athletes’ fatigue states during the planning and programming process.&lt;/p&gt;
&lt;p&gt;Like I said, we can use an athlete’s ACWR value to approximate their preparedness, with their acute workload representing “fatigue” and their chronic workload representing “fitness.” An ACWR ~ 1 represents training at a “normal” level, whereas values &amp;gt; 1 and &amp;lt; 1 represent supranormal and below-normal training, respectively. It isn’t necessarily wrong for the athlete’s ACWR value to move into either range; things only become problematic when the athlete is over-fatigued (extremely high ACWR or extended periods &amp;gt; 1) or under-prepared for the rigors of competition (extended periods of detraining / ACWR values &amp;lt; 1). Typically, the “sweet spot” for ACWR is somewhere between 0.8 to 1.5 (the red lines in the plots below). Feel free to check out &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Gabbett_2016&#34;&gt;3&lt;/a&gt;]&lt;/span&gt; for a much deeper conversation on this topic.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tl-snip.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Plotting an athlete’s ACWR alongside their daily and weekly training loads can give us an at-a-glance idea of whether our training loads have been appropriate. In the moment, we can use this data along with other monitoring data (e.g. jumps, questionnaires) to modify an athlete’s training. Retrospectively, we can leverage ACWRs and other monitoring data to understand what went wrong (no program is ever perfect) and how we can improve our training program in subsequent seasons. For instance, in the image above it should be pretty clear this athlete’s load profile is the opposite of what we want–a dense pre-season gives way to drastically reduced training loads during non-conference play that result in de-training. As he enters conference play, several double headers and a dense practice schedule (along with some punishment sessions for poor performance…and appearing tired 🤔) lead to a spike in his ACWR followed by an extended period &amp;gt; 1. His load is reduced in an attempt to manage his fatigue from the punishment…week, but he’s hit with another large spike in training (another punishment session) and a heavy week of training leading into the conference tournament. The team lost in the first round.&lt;/p&gt;
&lt;p&gt;It probably wouldn’t surprise you to know that this athlete’s jumps fell during pre-season, rose during non-conference play, then &lt;em&gt;plummeted&lt;/em&gt; during conference play and prior to the conference tournament. His mood questionnaire followed a similar pattern. I wasn’t working with this team at the time, so the best I can do is provide feedback during the planning phase for the next season. More on what that feedback might look like in another blog post. Today, we’re going to stay focused on ACWRs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-extremely-hot-topic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Extremely Hot Topic&lt;/h2&gt;
&lt;p&gt;While the use of ACWR values in athlete monitoring has become very popular in recent years, some recent papers have highlighted the shortcomings of the original ACWR calculation. First, fitness/fatigue degradation aren’t linear–they instead decline in a nonlinear fashion, with fatigue decaying at a faster rate than fitness &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-DeWeese_2015&#34;&gt;1&lt;/a&gt;,&lt;a href=&#34;#ref-Murray_2016&#34;&gt;8&lt;/a&gt;]&lt;/span&gt;. Further, the original ACWR calculation assigned the same relative weight to all training load values in both the acute and chronic rolling windows (that is, the load experienced yesterday carries the same weight as a training session from four weeks ago), when it stands to reason that more recently accumulated training will have a greater impact on an athlete’s fitness and fatigue. Enter exponentially-weighted moving averages (EWMA), where a decay function is applied to give greater weight to recently-completed training. At least one paper &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Murray_2016&#34;&gt;8&lt;/a&gt;]&lt;/span&gt; has shown greater sensitivity for EWMA to detect elevated injury risk when compared to the traditional moving average approach.&lt;/p&gt;
&lt;p&gt;A second hot topic as of late is “mathematical coupling” of the acute and chronic workload values. In case you’re scratching your head like I was the first time I saw the term “mathematical coupling” (I haven’t had a math class in 11 years…and we won’t get started on my quantitative reasoning score on the GRE), &lt;a href=&#34;https://en.wikipedia.org/wiki/Coupling_(probability)&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt; has a pretty good explanation of the concept. Basically, by including the acute workload window (the previous 3 - 7 days) in the chronic workload window (the last 21 - 28 days), we create a spurious correlation between the two values that mis-represents the athlete’s “true” ACWR &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Lolli_2017&#34;&gt;6&lt;/a&gt;]&lt;/span&gt;. For ACWR &amp;lt; 1, coupled ACWR values are greater than their uncoupled counterparts. The ratios are the same at a value of 1, but uncoupled values become greater than coupled values when discussing ACWR &amp;gt; 1. Windt and Gabbett &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Windt_2018&#34;&gt;9&lt;/a&gt;]&lt;/span&gt; argue that while coupling or uncoupling the acute and chronic windows will lead to different interpretations of the ACWR,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Coupled-vs-uncoupled.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;the fundamental idea is the same: we should avoid large increases in workload, especially in athletes with relatively low chronic training (think returning to play from injury or pre-season camps in collegiate sports). And really, that’s the crux of it all: avoid extremely large changes in training load because you will either overfatigue or underprepare your athlete and increase their risk of injury while decreasing their preparedness. And instead of just focusing on ACWR, we need to consider absolute and relative changes in workload in conjunction with measures of the athletes’ training tolerance (mood-state questionnaires, HRV, resting heart rate, jump testing, etc.). This holistic approach to athlete monitoring will provide us far more context and actionable data than getting caught up on a single metric (the good ole “forest for the trees” approach).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-acwr-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating ACWR in R&lt;/h2&gt;
&lt;p&gt;With the history out of the way, let’s get to the meat and potatoes of what I want to discuss: using R to calculate ACWR.&lt;/p&gt;
&lt;p&gt;If you aren’t familiar with R, you should be. R is a programming language that has been primarily used by statisticians to perform a wide variety of analytical techniques. The power of R comes from its high extensibility via packages–code libraries built to solve a variety of problems. Packages range from data manipulation packages (dplyr, data.table) to plotting (ggplot2, plotly) to dashboard creation (shiny, shinydashboard, flexdashboard) to…well, you get the idea. Basically, if you can dream up an idea, there’s probably an R package for it. And in the off chance it doesn’t exist in R but exists in Python (some of the high-level machine learning stuff), there’s even a package to call Python code in R! Really, R is a jack-of-all-trades, master-of-most language.&lt;/p&gt;
&lt;p&gt;This means problems that are extremely difficult to solve in Excel can often be conquered through loading a package and calling a single-line function in R. As an example, below is the array formula required to calculate a coupled chronic EWMA in Excel. You &lt;em&gt;might&lt;/em&gt; need a magnifying glass.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Excel-array-formula.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://progressiveathleticperformance.com/free-downloads/&#34; target=&#34;_blank&#34;&gt;Adam Sullivan’s site&lt;/a&gt; for the spreadsheet I pulled this formula from.&lt;/p&gt;
&lt;p&gt;Contrast that monster with what you would call in R:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(TTR)
EMA(training_load, n = 1, ratio = 2/(1+28))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What I did in the above code block is call the &lt;code&gt;TTR&lt;/code&gt; package (a package built for financial analyses) and then call the exponential moving average function (EMA). &lt;code&gt;EMA&lt;/code&gt; requires three pieces of information: the data we’re applying the function to, the size of the window to apply the decay ratio to, and the decay ratio. These correspond to column “E”, the formula in column “G”, and cell R9 in Adam’s spreadsheet I linked above.&lt;/p&gt;
&lt;p&gt;Granted, things get a bit more complicated when dealing with multiple athletes, but I think you’ll agree with me that the R code is still substantially easier to follow than the Excel formula. In this next example, I’m using the package &lt;code&gt;data.table&lt;/code&gt; in addition to the &lt;code&gt;TTR&lt;/code&gt; and &lt;code&gt;zoo&lt;/code&gt; packages to calculate both a traditional ACWR (via simple moving averages) and an exponentially-weighted ACWR. Check back soon because I’ll have a video up that discusses the ins and outs of how to use &lt;code&gt;data.table&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Data&lt;/h3&gt;
&lt;p&gt;First, the &lt;a href=&#34;tl-data.csv&#34;&gt;data&lt;/a&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;season&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;training.date&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;athlete&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tl&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rhiannon Garcia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;632&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Shajee’a al-Amiri&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1035&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Brittany Valdez&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1264&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Kayla Mix&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1242&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mariana Lewis&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Nicole Tipton&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Marshelle Edwards&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;948&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cinoi Slovonsky&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Mariana Lewis&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1112&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-09-r-acwr-intro/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-than-one-way-to-skin-a-cat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More Than One Way to Skin a Cat&lt;/h2&gt;
&lt;p&gt;My data are stored in a data frame called tl.data. These are real values, but I’ve changed the names and dates to anonymize things. Anyway, the data frame has four columns: season, training.date, athlete, and tl. We aren’t concerned with the season column today, but will revisit it once we dive down the rabbit hole of subsetting data. For now, let’s start by calculating a 7:28 ACWR the “traditional” way–that is, coupled simple moving averages. Unfortunately, &lt;code&gt;TTR::SMA&lt;/code&gt; doesn’t implement the “partial” argument, so we have to turn to &lt;code&gt;zoo::rollapplyr&lt;/code&gt; instead. With &lt;code&gt;SMA&lt;/code&gt;, you have to wait for both rolling windows to accumulate before an ACWR is calculated:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;season&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;training.date&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;athlete&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tl&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;acute&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;chronic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SMA.ACWR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1264&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;740&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-21&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-22&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;528&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-23&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;720&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-24&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;696&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;707.1429&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-26&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;480&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;595.1429&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;489.4286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1330&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;536.2857&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With &lt;code&gt;rollapplyr&lt;/code&gt;, on the other hand, the function will average the available data until it reaches the specified window size:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;season&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;training.date&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;athlete&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tl&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;acute&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;chronic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SMA.ACWR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1264&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1264.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1264.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;740&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1002.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1002.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-21&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1002.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1002.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-22&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;528&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;883.5000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;883.5000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-23&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;720&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;850.8000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;850.8000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-24&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;709.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;709.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;696&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;707.1429&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;707.1429&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-26&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;480&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;595.1429&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;678.7500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8768219&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;489.4286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;603.3333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8112076&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Fall 2017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2017-08-28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Urja Chaudhry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1330&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;536.2857&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7933221&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now, if you want to wait for the windows to accumulate before returning an ACWR value, disregard what I just said about &lt;code&gt;rollapplyr&lt;/code&gt; and use &lt;code&gt;SMA&lt;/code&gt; instead. I’m honestly not sure what the “appropriate” method is in this case, but seeing as how &lt;code&gt;SMA&lt;/code&gt; would spend ~ 1/3 of the season accumulating before returning an ACWR (college sports, man), I prefer &lt;code&gt;rollapplyr&lt;/code&gt; in this case. This does leave us with the question of how to account for the athletes’ training states coming into the season, but I’ll leave that for another post.&lt;/p&gt;
&lt;p&gt;Anyway, here’s how I calculated the ACWR values above with &lt;code&gt;rollapplyr&lt;/code&gt;. You’ll notice I’m able to apply these functions to each individual athlete thanks to the &lt;code&gt;data.table&lt;/code&gt; package. I’ll have a video on how that works soon.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2018-06-12 edit:&lt;/strong&gt; I realized I left out a very important piece of information! If you’re looking to copy these functions in the provided dataset or your own data, you’ll need to enable data.table’s functionality on the data prior to using the functions below. That’s accomplished by calling:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tl.data &amp;lt;- data.table(tl.data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sorry for any confusion!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Packages needed
library(data.table)
library(ggplot2)
library(zoo)

#Calculate each variable
tl.data[, sma.acute := rollapplyr(tl, 7, mean, partial = TRUE), by = .(athlete)]
tl.data[, sma.chronic := rollapplyr(tl, 28, mean, partial = TRUE), by = .(athlete)]
tl.data[, sma.ACWR := sma.acute/sma.chronic]

#Plot the data
ggplot(tl.data, aes(x = training.date)) + geom_col(aes(y = tl)) + 
  geom_line(aes(y = sma.ACWR * 1000)) + 
  scale_y_continuous(sec.axis = sec_axis(~./1000, name = &amp;#39;SMA ACWR&amp;#39;)) + 
  theme_bw() + facet_wrap(~athlete) + 
  labs(title = &amp;#39;TL with SMA ACWR&amp;#39;, x = &amp;#39;Training Date&amp;#39;, y = &amp;#39;TL&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-09-r-acwr-intro/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, calling &lt;code&gt;rollapplyr&lt;/code&gt; is a bit of a handful compared to &lt;code&gt;SMA&lt;/code&gt;, but whatever. As you can see, we now have individual ACWRs using the “traditional” calculation method. How about using exponentially-weighted moving averages? Thankfully, this is a lot easier since we can use &lt;code&gt;EMA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Packages needed
library(data.table)
library(ggplot2)
library(TTR)

#Calculate each variable
tl.data[, ema.acute := EMA(tl, n = 1, ratio = 2/(1+7)), by = .(athlete)]
tl.data[, ema.chronic := EMA(tl, n = 1, ratio = 2/(1+28)), by = .(athlete)]
tl.data[, ema.ACWR := ema.acute/ema.chronic]

#Plot the data
ggplot(tl.data, aes(x = training.date)) + geom_col(aes(y = tl)) + 
  geom_line(aes(y = ema.ACWR * 1000)) + 
  scale_y_continuous(sec.axis = sec_axis(~./1000, name = &amp;#39;EMA ACWR&amp;#39;)) + 
  theme_bw() + facet_wrap(~athlete) + 
  labs(title = &amp;#39;TL with EMA ACWR&amp;#39;, x = &amp;#39;Training Date&amp;#39;, y = &amp;#39;TL&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-09-r-acwr-intro/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, if you want the EMA to accumulate before returning values, you would call:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tl.data[, ema.acute := EMA(tl, n = 7, ratio = 2/(1+7)), by = .(athlete)]
tl.data[, ema.chronic := EMA(tl, n = 28, ratio = 2/(1+28)), by = .(athlete)]
tl.data[, ema.ACWR := ema.acute/ema.chronic]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I should point out these two methods &lt;strong&gt;&lt;em&gt;return different values&lt;/em&gt;&lt;/strong&gt;, unlike &lt;code&gt;SMA&lt;/code&gt; and &lt;code&gt;rollapplyr&lt;/code&gt;. This is because of the decay ratio in &lt;code&gt;EMA&lt;/code&gt;. So don’t use different window sizes (n = 1 vs. n = 7 or n = 28) interchangably. And be sure to report how you’re calculating your windows!&lt;/p&gt;
&lt;p&gt;Let’s compare our ACWR values for a single athlete. I’ve included ACWRs for the traditional method and both methods of EMA discussed above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Alternate EMA calculation
tl.data[, ema.acute.alt := EMA(tl, n = 7, ratio = 2/(1+7)), by = .(athlete)]
tl.data[, ema.chronic.alt := EMA(tl, n = 28, ratio = 2/(1+28)), by = .(athlete)]
tl.data[, ema.ACWR.alt := ema.acute.alt/ema.chronic.alt]

#Shift the ACWR values down one day to accurately reflect the effect of the previous day&amp;#39;s training
tl.data[, c(&amp;#39;sma.lagged&amp;#39;,&amp;#39;ema.lagged&amp;#39;, &amp;#39;ema.alt.lagged&amp;#39;) := 
          shift(.SD, n = 1, type = &amp;#39;lag&amp;#39;, fill = NA), 
        .SDcols = c(&amp;#39;sma.ACWR&amp;#39;,&amp;#39;ema.ACWR&amp;#39;, &amp;#39;ema.ACWR.alt&amp;#39;), by = .(athlete)]

#Plot the results
ggplot(tl.data[athlete == &amp;#39;Urja Chaudhry&amp;#39;], aes(x = as.Date(training.date), group = 1)) + 
  geom_col(aes(y = tl)) + geom_line(aes(y = sma.lagged * 1000, colour = &amp;#39;SMA&amp;#39;)) + 
  geom_line(aes(y = ema.lagged * 1000, colour = &amp;#39;EMA&amp;#39;)) + 
  geom_line(aes(y = ema.alt.lagged * 1000, colour = &amp;#39;EMA alt&amp;#39;)) +
  scale_y_continuous(sec.axis = sec_axis(~./1000, name = &amp;#39;ACWR&amp;#39;)) + 
  labs(title = &amp;#39;ACWR Comparison&amp;#39;, x = &amp;#39;Training Date&amp;#39;, y = &amp;#39;TL&amp;#39;, colour = &amp;#39;Method&amp;#39;) + 
  theme_bw() + theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-09-r-acwr-intro/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;2100&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hopefully you notice there are some pretty stark diffferences between the SMA and EMA methods, with the differences in the two EMA approaches mostly disappearing by day 45. This echoes what Windt and Gabbett &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Windt_2018&#34;&gt;9&lt;/a&gt;]&lt;/span&gt; say at the end of their editorial: explicity detail how you’re calculating your ACWRs!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coupled-vs.-uncoupled&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coupled vs. Uncoupled&lt;/h2&gt;
&lt;p&gt;If you’ve read Windt and Gabbett’s editorial &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Windt_2018&#34;&gt;9&lt;/a&gt;]&lt;/span&gt;, they provide an equation to convert coupled ACWRs to uncoupled ACWRs:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Uncoupled ACWR = \frac{3 * Coupled ACWR}{4 - Coupled ACWR}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This post is pretty long in the tooth at this point, so I’m only going to apply this function to my original EMA calculation (n = 1). The same functions will apply for the other ACWR methods.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Uncoupled EMA ACWR calculation
tl.data[, uncoupled.ema := (3 * ema.lagged)/(4 - ema.lagged)]

#Plot the results
ggplot(tl.data[athlete == &amp;#39;Urja Chaudhry&amp;#39;], aes(x = as.Date(training.date), group = 1)) +
  geom_col(aes(y = tl)) + geom_line(aes(y = ema.lagged * 1000, colour = &amp;#39;Coupled&amp;#39;)) +
  geom_line(aes(y = uncoupled.ema * 1000, colour = &amp;#39;Uncoupled&amp;#39;)) +
  scale_y_continuous(sec.axis = sec_axis(~./1000, name = &amp;#39;ACWR&amp;#39;)) +
  labs(title = &amp;#39;Coupled vs. Uncoupled&amp;#39;, x = &amp;#39;Training Date&amp;#39;, y = &amp;#39;TL&amp;#39;, colour = &amp;#39;Method&amp;#39;) +
  theme_bw() + theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-09-r-acwr-intro/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;2100&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And that’s it. There are some definite differences in the two methods, so again, be consistent in how you choose to perform your analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;ggplot code notwithstanding (that stuff gets to be a mess after a while), hopefully this post highlighted the fact that a few lines of R code can accomplish the same thing as lines and lines of Excel array formulas. The syntax might look a little scary at first, but once you get the hang of it, you can pound out extremely complex analysis code in a few minutes. From there, you can create a report or a dashboard that will allow you to drill down into your data.&lt;/p&gt;
&lt;p&gt;Something also worth mentioning is the time required to update an Excel spreadsheet vs. using a script in R. Excel does not play nice with large data sets that contain multiple sets of array formulas. As you add more data and the files get larger, they take longer to open, adding new training data causes the workbook to hang, and you’re &lt;em&gt;never quite sure&lt;/em&gt; if saving the workbook is going to save your additions or cause Excel to crash. Contrast that with R: I have eight years of men’s soccer RPE data. There are ~ 35,000 records, 89 athletes, and 16 seasons (fall/spring). Running the above EMA calculations (with n = 1) on the data took ~ 0.2 seconds. I’m pretty sure Excel would literally cause my computer to catch on fire if I tried that in a workbook. So while there’s some learning required on the front end, the benefits far outweigh the costs.&lt;/p&gt;
&lt;p&gt;If you do want to stay in the Microsoft ecosystem, though, you can have your cake and eat it too if you use Power BI, Microsoft’s business insights software. Think of it as a suped up version of Excel that can actually handle large amounts of data without exploding. Better yet, it has R code support, so you can actually import your data through Power BI and execute code that returns your data with all the necessary calculations. I’ve successfully used that approach to create dashboards for a few coaches over the last few years. I’ll (eventually) have a video up walking you through that process.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bi-example.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As for which ACWR method is “correct,” ¯\_(ツ)_/¯. I tend to agree with Windt and Gabbett &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-Windt_2018&#34;&gt;9&lt;/a&gt;]&lt;/span&gt; that we shouldn’t get lost in the minutiae of simple vs. exponential vs. coupled vs. uncoupled vs. whatever else this field comes up with. At the end of the day, we need to prescribe training loads in a logical fashion so we don’t completely break our athletes: High chronic workloads need to be built slowly over time to prepare our athletes for the worst-case scenario (for us a few years ago, that was three double overtime games in 12 days against teams who all finished in the top 15), and we need to monitor how our athletes are tolerating training through a combination of physical and psychological assessments. Let’s worry about the small problems once we get the basics down, yeah?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code-recap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code Recap&lt;/h2&gt;
&lt;p&gt;To recap the code I used in this post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Required packages
library(data.table)
library(ggplot2)
library(TTR)
library(zoo)

# Enabling data.table functionality
tl.data &amp;lt;- data.table(tl.data)

# Coupled ACWR via simple moving averages
tl.data[, sma.acute := rollapplyr(tl, 7, mean, partial = TRUE), by = .(athlete)]
tl.data[, sma.chronic := rollapplyr(tl, 28, mean, partial = TRUE), by = .(athlete)]
tl.data[, sma.ACWR := sma.acute/sma.chronic]

# Alternatively
tl.data[, sma.acute := SMA(tl, 7), by = .(athlete)]
tl.data[, sma.chronic := SMA(tl, 28), by = .(athlete)]
tl.data[, sma.ACWR := sma.acute/sma.chronic]

# Coupled ACWR via exponentially-weighted moving averages
tl.data[, ema.acute := EMA(tl, n = 1, ratio = 2/(1+7)), by = .(athlete)]
tl.data[, ema.chronic := EMA(tl, n = 1, ratio = 2/(1+28)), by = .(athlete)]
tl.data[, ema.ACWR := ema.acute/ema.chronic]

# Alternatively,
tl.data[, ema.acute := EMA(tl, n = 7, ratio = 2/(1+7)), by = .(athlete)]
tl.data[, ema.chronic := EMA(tl, n = 28, ratio = 2/(1+28)), by = .(athlete)]
tl.data[, ema.ACWR := ema.acute/ema.chronic]

# Uncoupling the ACWR
tl.data[, uncoupled.ema := (3 * ema.lagged)/(4 - ema.lagged)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me know if you have any questions! Shoot me an email: &lt;a href=&#34;mailto:samsperformancetraining@gmail.com&#34;&gt;samsperformancetraining@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-DeWeese_2015&#34;&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;em&gt;DeWeese BH, Hornsby G, Stone M, Stone MH. &lt;/em&gt;The training process: Planning for strengthpower training in track and field. Part 1: Theoretical aspects. Journal of Sport and Health Science 2015; 4: 308–317 Available from: &lt;a href=&#34;https://doi.org/10.1016/j.jshs.2015.07.003&#34;&gt;https://doi.org/10.1016/j.jshs.2015.07.003&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Ehrmann_2016&#34;&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;Ehrmann FE, Duncan CS, Sindhusake D, Franzsen WN, Greene DA. &lt;/em&gt;GPS and injury prevention in professional soccer. Journal of Strength and Conditioning Research 2016; 30: 360–367 Available from: &lt;a href=&#34;https://doi.org/10.1519/jsc.0000000000001093&#34;&gt;https://doi.org/10.1519/jsc.0000000000001093&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Gabbett_2016&#34;&gt;
&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;em&gt;Gabbett TJ. &lt;/em&gt;The traininginjury prevention paradox: Should athletes be training smarterandharder? British Journal of Sports Medicine 2016; 50: 273–280 Available from: &lt;a href=&#34;https://doi.org/10.1136/bjsports-2015-095788&#34;&gt;https://doi.org/10.1136/bjsports-2015-095788&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Hulin_2013&#34;&gt;
&lt;p&gt;&lt;sup&gt;4&lt;/sup&gt; &lt;em&gt;Hulin BT, Gabbett TJ, Blanch P, Chapman P, Bailey D, Orchard JW. &lt;/em&gt;Spikes in acute workload are associated with increased injury risk in elite cricket fast bowlers. British Journal of Sports Medicine 2013; 48: 708–712 Available from: &lt;a href=&#34;https://doi.org/10.1136/bjsports-2013-092524&#34;&gt;https://doi.org/10.1136/bjsports-2013-092524&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Hulin_2015&#34;&gt;
&lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; &lt;em&gt;Hulin BT, Gabbett TJ, Lawson DW, Caputi P, Sampson JA. &lt;/em&gt;The acute:Chronic workload ratio predicts injury: High chronic workload may decrease injury risk in elite rugby league players. British Journal of Sports Medicine 2015; 50: 231–236 Available from: &lt;a href=&#34;https://doi.org/10.1136/bjsports-2015-094817&#34;&gt;https://doi.org/10.1136/bjsports-2015-094817&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Lolli_2017&#34;&gt;
&lt;p&gt;&lt;sup&gt;6&lt;/sup&gt; &lt;em&gt;Lolli L, Batterham AM, Hawkins R, Kelly DM, Strudwick AJ, Thorpe R, Gregson W, Atkinson G. &lt;/em&gt;Mathematical coupling causes spurious correlation within the conventional acute-to-chronic workload ratio calculations. British Journal of Sports Medicine 2017; bjsports–2017–098110 Available from: &lt;a href=&#34;https://doi.org/10.1136/bjsports-2017-098110&#34;&gt;https://doi.org/10.1136/bjsports-2017-098110&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Malone_2018&#34;&gt;
&lt;p&gt;&lt;sup&gt;7&lt;/sup&gt; &lt;em&gt;Malone S, Hughes B, Doran DA, Collins K, Gabbett TJ. &lt;/em&gt;Can the workloadinjury relationship be moderated by improved strength, speed and repeated-sprint qualities? Journal of Science and Medicine in Sport 2018; Available from: &lt;a href=&#34;https://doi.org/10.1016/j.jsams.2018.01.010&#34;&gt;https://doi.org/10.1016/j.jsams.2018.01.010&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Murray_2016&#34;&gt;
&lt;p&gt;&lt;sup&gt;8&lt;/sup&gt; &lt;em&gt;Murray NB, Gabbett TJ, Townshend AD, Blanch P. &lt;/em&gt;Calculating acute:Chronic workload ratios using exponentially weighted moving averages provides a more sensitive indicator of injury likelihood than rolling averages. British Journal of Sports Medicine 2016; 51: 749–754 Available from: &lt;a href=&#34;https://doi.org/10.1136/bjsports-2016-097152&#34;&gt;https://doi.org/10.1136/bjsports-2016-097152&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Windt_2018&#34;&gt;
&lt;p&gt;&lt;sup&gt;9&lt;/sup&gt; &lt;em&gt;Windt J, Gabbett TJ. &lt;/em&gt;Is it all for naught? What does mathematical coupling mean for acute:Chronic workload ratios? British Journal of Sports Medicine 2018; bjsports–2017–098925 Available from: &lt;a href=&#34;https://doi.org/10.1136/bjsports-2017-098925&#34;&gt;https://doi.org/10.1136/bjsports-2017-098925&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
